"Machine Learning toolkit loaded"
"Q ML Training Client Process running on port 6001 [websocket mode]"
Using TensorFlow backend.
/home/foorx/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/foorx/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/foorx/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/foorx/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/foorx/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/foorx/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
"Connected to kdb master in cloud!"
"Automatic ML model retraining enabled!"
"Rolling Launch Control Model Trainer Up and Ready"
"Not using train test split!"
"Training GPS speed prediction model"
Training using KDB+ input!
Using PCA!
principalComponents:
[[ 0.02538003 -0.03177555  0.04559826 -0.03792274 -0.07377068  0.09732531
   0.41168592 -0.3205678  -0.03814009 -0.06048385  0.07468665 -0.21072224
   0.02775147 -0.04650529  0.38848046  0.41129968  0.40510423  0.40674779]
 [-0.00233922  0.00685093  0.04966499  0.45219126  0.14329593 -0.37591493
   0.08259703 -0.05357064  0.44958168  0.12683661 -0.40454377  0.11501343
  -0.41281527  0.16122009  0.14792916  0.02696723  0.01617495  0.08420799]
 [-0.41853732  0.41260865 -0.14008774  0.07641422 -0.51659923 -0.0891462
  -0.02650869  0.01539171  0.08520033 -0.52426498 -0.09839748 -0.22475569
  -0.0767028  -0.00690584 -0.0072794  -0.03343739 -0.06109785 -0.01450608]
 [-0.56601616  0.56818969  0.05000941 -0.06279788  0.37514262  0.10605211
   0.04460087 -0.05156356 -0.0625915   0.3455036   0.09522671  0.21205574
   0.05119095  0.07283824  0.02482947  0.05332934  0.06655987  0.02391122]
 [-0.05799603  0.07373813  0.33955187  0.02376523  0.06564442 -0.20003425
  -0.02937035  0.08094042  0.00520509  0.18678288 -0.12714682 -0.36417582
   0.0263447  -0.792692   -0.02705002 -0.07910386 -0.0423749   0.04535791]
 [ 0.00685269  0.02408832  0.89036717 -0.04712845 -0.13631385  0.06475046
   0.02115039  0.18438937 -0.03537023 -0.20836594  0.02708403  0.21949086
  -0.02977551  0.21933713  0.02903729  0.00220635 -0.0082411   0.06073412]
 [ 0.00152826  0.0015114   0.015668   -0.21683622 -0.0601732  -0.51644171
  -0.02127855 -0.32973427 -0.21710912 -0.07178725 -0.42039097  0.31738195
   0.44863032  0.00291496 -0.14837932  0.0543632   0.13078683 -0.01083224]
 [ 0.01377391 -0.02175561 -0.05010042  0.15465993 -0.17048681  0.18266034
  -0.00832398 -0.31359197  0.14453481 -0.13847472  0.23845296  0.67073917
  -0.14257838 -0.49093066  0.01844362 -0.00891053 -0.05890803  0.00445216]
 [ 0.00484786  0.00778415 -0.22811462 -0.03860583 -0.04230907 -0.12893431
   0.17826598  0.79535762 -0.0553407  -0.05011416 -0.0556281   0.32093544
   0.0410667  -0.166597    0.05070843  0.16286227  0.25804888  0.16373398]]
Number of hidden layers: 1
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 2
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 3
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 4
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 5
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 6
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 7
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 8
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 9
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 10
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 11
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 12
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 13
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 14
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 15
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 16
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 17
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 18
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 19
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 20
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)


Testing all kernels...
testX
            0         1         2  ...         6         7         8
0    2.392462  0.018621 -0.560262  ... -0.077570  0.497595  0.631506
1   -1.584484 -1.503878 -1.323770  ... -0.304189  0.374329  0.920607
2   -2.319494 -4.586502 -0.624723  ...  1.569732  0.280313  1.207835
3   -2.374951 -5.159608 -1.442012  ...  2.535521  0.121395  1.326143
4   -1.996943 -6.183839 -0.374233  ...  2.625592 -0.146505  1.028200
5   -2.238143 -5.893740 -0.355142  ...  1.929148 -0.647487  0.776529
6   -2.068168 -5.487920 -1.851734  ...  1.760126 -1.128334  0.589310
7   -2.259355 -3.804018  0.733963  ...  1.370121 -1.301448  0.626078
8   -2.092085 -3.355708 -1.088056  ...  0.899724 -1.273712  0.516116
9   -2.404963  1.514406  1.023645  ...  0.038289 -1.004260  1.149622
10  -2.758181  2.216438  3.411069  ... -0.284380 -1.072563  1.126050
11  -2.642964  2.306587  1.468009  ... -0.502036 -0.806391  1.141526
12  -2.654728  2.499092  0.325550  ... -0.458997 -0.536017  1.180583
13  -2.745889  2.762969  0.551743  ... -0.717088 -0.338846  1.256008
14  -2.920951  3.610009  0.169977  ... -0.747923 -0.266038  1.207640
15  -3.077460  4.335043 -1.141920  ... -0.803623 -0.334066  1.205701
16  -3.273132  4.765698  0.228107  ... -0.342215 -0.346622  1.433324
17  -3.203188  4.780510 -0.571441  ... -0.188930 -0.368419  1.456656
18  -3.216726  4.639692 -0.349179  ... -0.090690 -0.268671  1.467305
19  -3.139295  3.517559 -2.022732  ... -0.350354 -0.566005  1.264642
20  -2.027662 -0.699110 -0.459782  ...  0.554161 -0.823163  1.124864
21  -1.891220 -0.490448 -1.168193  ... -0.209014 -0.819491  1.000701
22  -1.443667 -0.327522 -1.045735  ... -0.085639 -0.765520  1.054393
23  -0.436820 -0.196168 -1.358430  ... -0.061268 -0.922350  1.068019
24   0.760622  0.017975  0.378194  ... -0.101903 -1.099581  0.759776
25   1.140452 -0.049393 -0.460338  ... -0.035384 -1.161488  0.853454
26   2.152835 -0.019232 -0.609026  ...  0.223647 -0.731037  0.722103
27   2.520986  0.090418 -0.403010  ...  0.314457 -0.456353  0.736331
28   2.787381 -0.214381 -1.209401  ... -0.542193  0.050588  0.624378
29   2.508816 -0.190893  0.337279  ...  0.045175  0.684383  1.065787
30   2.627373 -0.113838 -0.582833  ... -0.261729 -0.016308  0.751136
31   2.470724  0.196838 -0.862615  ... -0.039529 -0.144720  0.820378
32   2.364479  0.422115 -0.598682  ...  0.306446  0.766446  1.088907
33   2.531019  0.051901 -0.219187  ... -0.308778  0.138627  0.758251
34   2.485377  0.135907  2.546290  ... -0.156148 -0.406163  0.567775
35   2.502204  0.042543  0.559501  ... -0.073658 -0.020697  0.687250
36   1.050575 -0.364427  1.089790  ... -0.668109 -0.195461  0.834819
37  -0.428817 -0.777826  1.252114  ... -1.172501  0.124458  0.772639
38  -1.516262 -0.783392  1.310556  ... -1.429417  0.311868  0.629444
39   0.398304  0.602214 -1.704783  ...  0.316499  1.404074  1.664671
40   1.280041  0.447257 -1.837392  ...  0.491251  1.711488  1.570251
41   1.977333  0.659762  0.079369  ...  0.409377  0.664250  1.081473
42   2.258364  0.307126 -0.567065  ...  0.227701  0.736475  1.028621
43   2.709184  0.220583  1.137707  ... -0.109829  0.320156  0.759348
44   2.790297  0.302292  0.711430  ... -0.551207 -0.935585  0.236542
45   2.838659 -0.139975 -0.020280  ... -0.456858 -0.494441  0.407556
46   2.592268  0.171730  2.708360  ... -0.397583 -0.217033  0.514592
47   2.502845  0.545851  0.657589  ... -0.012758  0.238972  0.662833
48   2.694629  0.251302  0.117399  ... -0.220236  0.121360  0.606059
49   2.662765  0.584543  0.124296  ... -0.173501 -0.716461  0.246748
50   2.794749  0.287398 -0.754644  ... -0.794108 -0.224901  0.272237
51   2.181652 -0.104791 -1.041669  ... -0.281018  0.485066  0.516405
52  -2.303441 -2.859102  0.355995  ...  0.308205  0.951607  1.204975
53  -2.514097 -4.315153 -1.440866  ...  1.239199  0.500189  1.169086
54  -2.181945 -5.517032 -1.613318  ...  2.294879  0.081450  1.224123
55  -3.206585  4.564374  1.034368  ... -0.356263 -0.499873  1.337107
56  -3.186147  4.546309 -0.693962  ... -0.252282 -0.274310  1.413722
57  -3.207676  3.254417 -2.271137  ... -0.475997 -0.714435  1.261119
58  -3.251743  2.120320 -3.957096  ... -1.076266 -1.028747  1.077929
59  -1.979071  1.861911 -3.552736  ... -1.517023 -0.165000  1.045656
60   1.522573  1.235941  1.443751  ... -1.639305  1.005477  0.672813
61   1.997296  0.771425 -0.029456  ... -1.620921  0.991943  0.653244
62   2.663265  0.546358  0.636993  ... -1.623892  0.651053  0.332144
63   2.902649  0.267752 -0.068678  ... -1.696905  0.017039  0.093953
64   2.882277  0.257472 -0.671188  ... -0.675669 -0.054631  0.293777
65   2.880436  0.213002  1.882244  ... -0.504369 -1.100590 -0.103018
66   3.123278 -0.300111  0.654842  ... -0.690826 -0.196242  0.081172
67   3.408373 -0.593121  1.019893  ... -0.399213 -0.727504 -0.178949
68   3.331266 -0.359003  0.700066  ... -0.615706 -1.243135 -0.386804
69   3.385100 -0.413018 -0.779179  ... -0.747061 -1.213958 -0.444545
70   3.021502 -0.353910  0.810385  ... -0.565602 -0.792629 -0.161706
71   3.150853 -0.010206  0.188137  ... -0.294852 -0.957809 -0.236818
72   2.900979  0.700052 -0.317971  ... -0.446253 -1.072498 -0.428751
73  -0.672568  0.795474 -0.255048  ... -0.544717 -0.246942  0.890928
74  -1.949663 -0.412429  0.330698  ... -1.084136  0.678673  1.019466
75   2.537444 -0.237224  0.750199  ... -0.350133  0.082535  0.786183
76   2.441364 -0.018527  1.362638  ... -0.034747  0.401185  0.962165
77   2.515782  0.050868 -0.698305  ... -0.130005  0.371873  0.903099
78   2.371646  0.210455 -0.447339  ...  0.054680  0.500989  1.045620
79   2.675346 -0.220349 -0.355990  ... -0.338972  0.749947  0.894693
80   2.690108 -0.182667 -1.491429  ... -0.066671  0.190420  0.702973
81   0.100739 -0.654715  2.427740  ... -1.108272 -0.230429  0.706532
82  -0.609437 -0.667704  0.967299  ... -1.127112 -0.031823  0.693070
83  -1.793976 -0.822213  1.032932  ... -1.330776  0.216005  0.580171
84  -1.986123 -0.520407  1.112605  ... -1.227296 -0.015472  0.374098
85  -1.977639 -0.411381  0.104871  ... -1.300540 -0.047159  0.278318
86  -2.448789  0.904217  1.917326  ... -1.456905 -0.315837  0.241820
87  -2.508067  1.744916  0.248617  ... -1.413515 -0.140046  0.270250
88  -2.344561  0.250497 -4.481187  ... -0.356261  0.148155  0.830761
89  -2.636600  0.101959 -1.372632  ... -0.113094  0.719826  1.113230
90   2.919868  0.059489 -1.449970  ... -1.327862  0.231677  0.266657
91   3.018640  0.014808  1.765291  ... -0.797148 -0.488635 -0.042358
92   3.163106 -0.539597  0.812377  ... -0.677708 -0.046550  0.130557
93   3.405094 -0.538648  1.067878  ... -0.553348 -0.282058 -0.022117
94   3.371595 -0.459102  1.220376  ... -0.464323 -1.332340 -0.316481
95   3.332739 -0.237228 -0.338916  ... -0.143996 -1.559238 -0.446536
96   3.041616 -0.171698  1.080024  ... -0.245575 -1.016150 -0.239529
97   3.160455  0.000291  0.184360  ... -0.328976 -0.324384 -0.036977
98   2.999878  0.430775  0.145098  ... -0.266296 -0.497159 -0.132591
99  -0.243172  0.711153 -0.002470  ... -0.525160 -0.359115  0.792537
100 -1.825266 -0.453573 -1.006424  ... -1.222142  0.751767  1.062744
101 -2.212999 -0.546841  1.863479  ... -1.047026  0.316239  0.921091
102 -2.089982 -0.590803  0.023863  ... -1.123451  0.441393  0.897165
103 -2.112134 -0.521720  0.107641  ... -1.025928  0.261495  0.857615
104 -2.134762 -0.614044 -0.135047  ... -0.936459  0.024899  0.864539
105  2.535419 -0.038536 -0.613795  ...  0.206338 -0.502159  0.696987
106  2.558355  0.369777 -0.207897  ...  0.098597 -0.331990  0.657284
107  2.543792  0.311595 -1.663407  ...  0.035243 -0.004966  0.811282
108  2.428671 -0.030816  1.162907  ...  0.003476  0.448728  1.036663
109  2.464127  0.197843 -0.765089  ... -0.039580 -0.148757  0.820520
110  2.486969 -0.014822 -0.778823  ... -0.077079  1.071431  1.092803
111  2.430929  0.268965 -0.355701  ...  0.039882 -0.086374  0.765477
112  2.702778 -0.022302 -1.265053  ... -0.027320  0.059174  0.684840
113  0.369528 -0.631911  2.285431  ... -1.014092 -0.274854  0.787337
114 -0.421907 -0.778896  1.150463  ... -1.172463  0.128707  0.772429
115 -1.509178 -0.784489  1.206355  ... -1.429378  0.316223  0.629229
116 -1.914179 -0.767308  1.218179  ... -1.328417  0.052741  0.387356
117 -1.964510 -0.491320  0.017495  ... -1.393957 -0.028163  0.305625
118 -2.373613  0.622739  1.831071  ... -1.695132 -0.188589  0.145882
119  1.994156  0.692023 -2.081551  ...  0.493278  0.736733  1.180931
120  2.448923  0.306416  1.891928  ... -0.046320  0.645500  0.887125
121  2.704413  0.093003  1.024129  ... -0.185798  0.317932  0.737515
122  2.772122  0.420495  0.570473  ... -0.433646 -0.907803  0.275548
123  2.802310 -0.036138 -0.189036  ... -0.374946 -0.495403  0.422447
124  2.866062  0.158466 -1.200136  ... -0.550953 -0.067810  0.492948
125  2.577638  0.497373  0.746538  ... -0.443128 -0.525083  0.377815
126  2.815686  0.140936 -0.051036  ... -0.536938 -0.482161  0.231413
127  2.806117  0.207928 -0.731767  ... -0.874617 -0.259034  0.235399
128  2.072754  0.147958 -0.464728  ... -0.104766  0.283319  0.515508
129 -1.724429 -1.896542 -1.455647  ... -0.178945  0.754126  1.090408
130 -2.321065 -4.878225 -0.036000  ...  1.861815  0.274116  1.223911
131 -2.316861 -5.386417 -1.562716  ...  2.348135  0.092259  1.180735
132 -2.349805 -5.849187 -0.084998  ...  2.536718 -0.210808  0.993194
133 -2.168421 -5.826534 -0.432353  ...  1.912822 -0.661796  0.733016
134  0.638339 -0.069974  0.711813  ... -0.001853 -1.052570  0.842270
135  1.106496 -0.065178 -0.638471  ... -0.078055 -0.907896  0.697482
136  1.997845 -0.211627 -0.539311  ...  0.023678 -0.167089  0.979699
137  2.549593 -0.069797 -0.748226  ...  0.144696 -0.967795  0.598973
138  2.524111  0.219450 -0.982583  ...  0.117178 -0.126844  0.848524
139  2.508690  0.057652 -0.032631  ... -0.259387 -0.101145  0.745602
140  2.679130 -0.364545 -0.370729  ... -0.161520  0.974123  1.136739
141  2.649890 -0.047898 -0.701842  ... -0.278048  0.059154  0.706174
142  2.391871  0.224225 -0.886532  ...  0.013153  0.320723  0.988031
143  2.372166  0.537280 -1.487540  ...  0.451392 -0.183846  0.798629
144  2.485509  0.063128  0.491046  ...  0.118023 -0.023341  0.712072
145  1.508340 -0.294075  0.676396  ... -0.373311  0.198029  0.900488
146 -0.208559 -0.633197  1.002322  ... -1.103116  0.001696  0.793830
147 -1.303428 -0.578393  1.223944  ... -1.431399  0.156988  0.562612
148 -1.802418 -0.853488  0.579144  ... -1.346896  0.119647  0.413636
149 -2.093579 -0.465710  1.741594  ... -1.407531 -0.177874  0.282827
150 -2.189089  0.198004  0.677696  ... -1.600703 -0.235327  0.186897
151 -2.360215  1.018820  0.397634  ... -1.520407 -0.271093  0.202841
152 -2.911016  3.712108  0.836551  ... -0.998053 -0.258142  0.482315
153 -3.034356  4.307945  0.750438  ... -1.549533  0.068476  0.303985
154  0.185979 -1.037127  2.422270  ... -0.339520 -1.334771  0.167693
155  0.299041 -0.565203 -0.093568  ... -0.596308 -1.819797 -0.029314
156 -0.170438 -0.587562 -0.284807  ... -0.220375 -1.561175  0.087308
157 -1.185735 -0.327466 -1.788170  ... -0.405739 -1.204957  0.379375
158 -1.640452  0.022662 -3.521596  ... -0.559650 -0.893166  0.491619
159 -2.432094  0.200140 -2.634314  ... -0.468531 -0.059628  0.820676
160 -2.535613  0.233007 -3.554174  ... -0.326971  0.382361  0.966970
161 -2.392248  0.354006 -0.139083  ...  0.614014  0.329494 -1.688046
162 -2.420206  0.386107 -0.210939  ...  0.471417  0.479365 -1.710843
163 -2.239799 -0.090798 -1.186825  ...  0.374181  0.439863 -1.734578
164 -2.161813 -0.147615  0.685969  ...  0.161080  0.575004 -1.700737
165 -1.990105 -0.067275 -0.347522  ...  0.078471  0.549088 -1.749315
166 -1.899385 -0.076984 -0.327524  ...  0.087171  0.457286 -1.747533
167 -1.786746 -0.413868  0.660247  ...  0.332132  0.653742 -1.662634
168 -1.753842 -0.548263  0.039275  ...  0.441791  0.480745 -1.662482
169 -2.035414 -0.208491 -2.420010  ...  0.534903  0.501860 -1.692404
170 -2.238640 -0.019547 -0.460177  ...  0.549642  0.524954 -1.629904
171 -2.297174  0.857524 -0.314219  ...  0.338860  0.641078 -1.688543
172 -2.339361  1.199962 -0.290436  ...  0.494328  0.646955 -1.681620
173 -2.515889  2.726687 -0.611643  ...  1.213972  0.337894 -1.595433
174 -2.788722  5.042977  3.036172  ...  2.183763  0.310044 -1.283559
175 -2.694776  5.247579  1.595045  ...  2.315623  0.409991 -1.190518
176 -2.467425  5.903332  3.468449  ...  2.906398  0.945964 -0.928200
177 -2.183059  6.080760  4.617376  ...  3.482007  1.019010 -0.761616
178 -2.021999  5.664467  3.399981  ...  3.797518  0.815041 -0.673274
179 -1.535517  3.533329  5.313188  ...  3.443954  0.624213 -0.616884
180 -1.587334  1.795373  2.806142  ...  2.629195  0.897529 -0.946798
181 -1.837842  1.150256  2.572135  ...  2.520712  0.871666 -0.972846
182 -1.939029  2.013787  3.818358  ...  3.239583  0.950609 -0.790963
183 -0.980931 -0.643759 -0.206488  ... -0.661020  0.297341 -0.517859
184 -0.728410 -0.525347 -0.521701  ... -0.383695 -0.004725 -0.603016
185 -0.548964 -0.610294 -0.576971  ... -0.210218 -0.162796 -0.617643
186 -0.194420 -0.531590 -1.422983  ... -0.001001 -0.423560 -0.641049
187 -0.088370 -0.258503  0.625620  ... -0.147500 -0.482232 -0.569615
188  0.055187 -0.146400 -0.818401  ...  0.151167  0.138243 -0.338389
189  0.384078 -0.121748 -0.647042  ... -0.055526 -0.375512 -0.554226
190  0.385895 -0.346635 -0.011593  ... -0.202242 -0.447170 -0.613521
191 -0.331737  0.389601 -0.847264  ... -0.681291  0.257732 -0.742227
192 -1.012455  2.676239  1.364247  ... -0.484656  0.068782 -0.644409
193  3.201758  0.496727 -0.708202  ...  0.470726  0.227247 -0.826995
194  2.185168 -0.278109  0.069129  ...  0.107258 -0.671730 -1.133476
195  0.273102 -1.317263  0.140767  ... -0.122007  0.951861 -0.656241
196 -0.172258 -0.402558  0.067667  ... -0.706080 -0.739194 -1.378798
197 -0.192820 -0.349926 -0.067406  ... -0.733212 -0.863859 -1.372298
198 -0.254759 -0.411059  0.107625  ... -0.517171 -0.478955 -1.241796
199 -0.004687 -0.474820 -1.574701  ... -0.376106 -0.358869 -1.151513
200 -0.014541  0.151903 -0.377111  ... -0.596594 -0.560296 -1.206106
201 -0.137459  0.219775  0.074086  ... -0.193878  0.323827 -0.808805
202 -0.169682 -0.222740  0.274766  ... -0.447095 -0.275241 -1.042663
203 -0.100800 -0.761886 -2.207129  ... -0.193810 -0.410616 -1.153412
204  2.824893  0.018380 -0.724286  ...  0.292373  0.096762 -1.008259
205  3.301834  0.048300 -0.700878  ...  0.518839  0.353972 -1.021157
206  3.257054  0.156058 -0.710940  ...  0.391240  0.003025 -1.047224
207  3.210320  0.305651 -1.529236  ...  0.716883  0.305893 -0.866092
208  3.226676  0.082772  0.328208  ...  0.363834 -0.123950 -1.082952
209  3.167719  0.334038 -0.669282  ...  0.694921  0.032830 -0.954240
210  3.213371  0.442305 -1.431895  ...  0.363271  0.547315 -0.900123
211  2.850091  0.659195 -2.149421  ...  0.478381  0.683880 -0.727566
212  2.553680  0.742254 -2.218923  ...  0.892753  1.258307 -0.453989
213  2.431545  0.622195 -0.396810  ...  0.858826  1.305537 -0.381368
214  2.663121  0.399731  0.195595  ...  0.920086  1.415506 -0.442426
215  2.799243  0.284025 -1.224513  ...  0.843074  1.418937 -0.481321
216 -2.521936 -0.828238 -1.615447  ...  0.209774  0.322044 -1.505835
217 -2.550709 -0.839299  0.649755  ...  0.066189  0.250634 -1.507499
218 -2.491538 -0.819005 -0.814482  ...  0.017765  0.362350 -1.503973
219 -2.459626 -0.949745  0.044819  ...  0.203588  0.452829 -1.507255
220 -2.509298 -0.860184  0.133154  ...  0.135948  0.383497 -1.498633
221 -2.466000 -0.716725 -0.740123  ...  0.110092  0.228624 -1.565741
222 -1.693778 -0.510630  0.664879  ... -0.459819  0.140115 -1.450291
223 -1.021869 -0.643367  0.041898  ... -0.999990  0.079039 -1.605159
224 -0.237099 -0.644624  0.494799  ... -1.509799  0.218601 -1.775184
225 -0.160749 -0.669746 -0.316571  ... -0.906839 -0.687497 -1.833005
226  0.346597 -0.688188 -1.398478  ... -0.149324 -1.183418 -1.669291
227  0.920728 -0.425631  0.298030  ...  0.061516 -1.257362 -1.440419
228  3.178806 -0.437794  0.830794  ... -0.670854 -0.646789 -0.090964
229  3.297790 -0.666547  1.070728  ... -0.335020 -1.158253 -0.197228
230  3.264831 -0.411851  0.743196  ... -0.396810 -0.978466 -0.222703
231  3.272050  0.344165  0.736018  ... -0.370321 -1.655488 -0.581205
232  3.133539 -0.162293  2.537887  ... -0.145980 -0.478893 -0.093353
233  2.914387  0.049190  0.789555  ...  0.081948 -0.136622  0.170285
234  3.020686  0.147278 -0.166311  ... -0.231066 -0.607856 -0.097853
235  2.900779  0.458634 -0.664805  ... -0.476306 -0.446579 -0.167509
236 -1.203613  0.575427  0.346499  ... -0.669851 -0.205222  0.878728
237 -1.897188 -0.513820 -0.926045  ... -1.116033  0.926846  1.062036
238 -2.201778 -0.631818  1.857758  ... -1.154342  0.427825  0.924853
239 -2.074609 -0.534491  0.009094  ... -1.153118  0.338884  0.859575
240 -2.107216 -0.536806 -0.000635  ... -1.128434  0.048298  0.824349
241 -2.111567 -0.865554  0.220488  ... -0.922554 -0.110951  0.833889
242 -2.086651 -0.932159 -0.734078  ... -0.919112 -0.006960  0.814979
243 -2.344098 -2.037594  1.855176  ... -0.406276 -0.201164  0.956481
244  2.570394  0.109128 -0.678831  ...  0.043663 -0.011385  0.886949
245  2.533316  0.261543 -0.722662  ... -0.177609 -0.585657  0.651110
246  2.617576 -0.229300 -1.396385  ...  0.019869  0.839270  1.088330
247  2.568440 -0.293328  0.697915  ... -0.486755  0.429196  0.857727
248  2.478425  0.177368 -0.598045  ... -0.051495  0.767899  0.993690
249  2.494329  0.016017 -0.056424  ...  0.021163  0.712875  1.053092
250  2.561373 -0.202578 -0.534444  ...  0.086256  0.769338  1.023414
251  2.518963  0.241554 -1.149824  ...  0.078893 -0.047342  0.684763
252 -0.123234 -0.689741  2.211246  ... -1.114298 -0.027722  0.830574
253 -0.808416 -0.730972  1.070730  ... -1.225186  0.103314  0.757706
254 -1.855064 -0.764701  1.126085  ... -1.318939  0.242070  0.547421
255 -1.980811 -0.641565  1.128386  ... -1.354170  0.097415  0.377968
256  2.440740  0.195401  1.787262  ...  0.003479 -0.066629  0.712892
257  2.865449  0.039717  0.672398  ... -0.344284 -0.635215  0.278339
258  2.821908  0.179347  0.215792  ... -0.782992 -0.923753  0.159659
259  2.786763  0.172575 -0.267291  ... -0.290525 -0.123428  0.501309
260  2.552866  0.537588 -0.063558  ... -0.012936  0.270319  0.659487
261 -2.175043 -5.786335 -1.204607  ...  2.547253 -0.302854  1.214043
262 -2.083580 -3.738772 -0.284010  ...  1.925972 -0.113239 -1.070314
263 -1.871546 -3.813847 -1.301460  ...  1.977939 -0.230054 -1.076969
264 -1.754078 -3.631789 -1.076362  ...  1.766096  0.012975 -1.104846
265 -1.730068 -3.369277 -0.668751  ...  1.486355  0.070497 -1.198950
266 -1.690341 -3.120008 -1.928008  ...  1.409356  0.088284 -1.119279
267 -1.733974 -2.399972  1.176987  ...  0.908771  0.112772 -1.186817
268 -1.599859 -1.930854 -0.602452  ...  0.196034  0.138429 -1.381277
269 -1.374610 -0.979076 -0.728732  ... -0.158701  0.496115 -1.440894
270 -1.085810 -0.980859 -3.302499  ... -0.415107  0.519619 -1.518886
271 -0.919015 -0.699338  2.599826  ...  0.754812  0.088608 -1.268311
272 -0.819135 -0.702377  1.481453  ...  0.577142  0.357166 -1.267186
273 -1.861055  0.193065 -0.595345  ...  0.469265  0.286720 -1.613837
274 -2.182221  0.487417 -1.996554  ...  0.360024  0.236677 -1.776530
275 -2.965393  0.775339 -0.243266  ...  0.373142  0.198622 -1.886873
276 -2.898710  1.205348 -2.021199  ...  0.351431  0.237256 -1.867881
277 -2.915523  0.668137 -1.827927  ...  0.069484  0.475405 -1.930891
278 -2.645713  0.236598 -0.445308  ... -0.058678  1.209273 -1.793582
279 -2.591509  0.093726 -1.318174  ... -0.238600  1.160890 -1.892334
280 -2.366145  0.363774  2.401342  ... -0.157695  1.276270 -1.800523
281 -2.175254 -0.358805  0.627504  ... -0.090051  1.136493 -1.918932
282 -2.241798 -0.815140  0.179723  ... -0.353832  0.833573 -1.991909
283 -2.298352 -1.075714 -2.589359  ... -0.221534  0.873711 -1.959596
284 -2.768197 -0.871335  0.732712  ... -0.047275  0.712643 -1.928848
285 -2.666867 -0.768671 -0.219482  ... -0.002307  0.704746 -1.912260
286 -2.687944  5.873984  2.928869  ...  4.062389  0.165465 -0.788142
287 -1.192561  4.607790  5.890254  ...  4.698379  0.968007 -0.298641
288 -2.123201  5.349181  5.467110  ...  5.072091  0.204431 -0.401086
289 -2.270060  5.522427  5.133097  ...  5.241898  0.779180 -0.315718
290 -2.688320 -0.692533 -1.159010  ...  0.302338 -0.214115 -0.324840
291 -2.594358 -0.496045 -0.767077  ...  0.267992  0.440898 -0.303819
292 -2.478032 -0.447098 -0.768842  ...  0.251196  0.508144 -0.323860
293 -2.002688 -0.014830 -0.711324  ...  0.211230  0.218045 -0.380182
294 -1.769865 -0.367852 -1.534780  ...  0.065398  0.038509 -0.397383
295 -1.951968  0.019743 -1.228728  ... -0.193663 -0.461167 -0.498188
296 -1.746896  0.158073 -2.654775  ... -0.129554 -0.411838 -0.560256
297 -1.003623 -0.281146 -0.296511  ...  0.254406  0.148206 -0.279796
298  0.222378 -0.244507  0.555341  ...  0.292724 -0.121869 -0.470786
299  1.266378 -0.124559 -0.835634  ...  0.171708 -0.648607 -0.992116
300 -0.525402 -0.228152  0.242975  ... -0.065539 -0.038306 -1.656416
301  0.285947 -0.164114 -1.390387  ...  0.044349 -0.176214 -1.585843
302 -2.100494  0.373808  1.867071  ...  0.481682  0.087599 -1.678843
303 -2.100371  0.320245  0.567494  ...  0.364529  0.218169 -1.745893
304 -2.181410  0.077829 -0.241082  ...  0.128995  0.473507 -1.830394
305 -2.296450  0.195993 -0.170941  ... -0.105941  0.653137 -1.837355
306 -2.595658 -0.087455 -2.100413  ...  0.174520  0.566114 -1.935950
307  1.994156  0.692023 -2.081551  ...  0.493278  0.736733  1.180931
308  2.448923  0.306416  1.891928  ... -0.046320  0.645500  0.887125
309  2.704413  0.093003  1.024129  ... -0.185798  0.317932  0.737515
310  2.772122  0.420495  0.570473  ... -0.433646 -0.907803  0.275548
311  2.802310 -0.036138 -0.189036  ... -0.374946 -0.495403  0.422447

[312 rows x 9 columns]
['default', 'tanh', 'tribas', 'hardlim', 'rbf(0.1)']

Layer: 1
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
723.7794409942906
RMSE:
26.90
Testing kernel: tribas
MSE:
722.9215632932285
RMSE:
26.89
Testing kernel: hardlim
MSE:
721.3206843075369
RMSE:
26.86
Testing kernel: rbf(0.1)
MSE:
723.1996112778062
RMSE:
26.89

Layer: 2
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
734.1210121716864
RMSE:
27.09
Testing kernel: tribas
MSE:
721.5548413807227
RMSE:
26.86
Testing kernel: hardlim
MSE:
742.2956828735416
RMSE:
27.25
Testing kernel: rbf(0.1)
MSE:
706.8299582370252
RMSE:
26.59

Layer: 3
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
751.023957650132
RMSE:
27.40
Testing kernel: tribas
MSE:
715.5607599572015
RMSE:
26.75
Testing kernel: hardlim
MSE:
727.8676666450204
RMSE:
26.98
Testing kernel: rbf(0.1)
MSE:
701.6928042297942
RMSE:
26.49

Layer: 4
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
722.0715693053196
RMSE:
26.87
Testing kernel: tribas
MSE:
724.6677827627352
RMSE:
26.92
Testing kernel: hardlim
MSE:
721.9353024827022
RMSE:
26.87
Testing kernel: rbf(0.1)
MSE:
708.9846823355261
RMSE:
26.63

Layer: 5
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
726.9902638974452
RMSE:
26.96
Testing kernel: tribas
MSE:
724.4274183602051
RMSE:
26.92
Testing kernel: hardlim
MSE:
729.989350852086
RMSE:
27.02
Testing kernel: rbf(0.1)
MSE:
848.4755005845221
RMSE:
29.13

Layer: 6
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
748.5540721476457
RMSE:
27.36
Testing kernel: tribas
MSE:
723.756035973656
RMSE:
26.90
Testing kernel: hardlim
MSE:
749.1322509155486
RMSE:
27.37
Testing kernel: rbf(0.1)
MSE:
851.8349122408903
RMSE:
29.19

Layer: 7
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
762.9559392554357
RMSE:
27.62
Testing kernel: tribas
MSE:
733.0780234218662
RMSE:
27.08
Testing kernel: hardlim
MSE:
747.7225378226568
RMSE:
27.34
Testing kernel: rbf(0.1)
MSE:
832.3130193446664
RMSE:
28.85

Layer: 8
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
800.1829930091934
RMSE:
28.29
Testing kernel: tribas
MSE:
733.612658373535
RMSE:
27.09
Testing kernel: hardlim
MSE:
764.0454219712507
RMSE:
27.64
Testing kernel: rbf(0.1)
MSE:
838.4611842711233
RMSE:
28.96

Layer: 9
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
758.3088495357772
RMSE:
27.54
Testing kernel: tribas
MSE:
746.9208096427965
RMSE:
27.33
Testing kernel: hardlim
MSE:
747.2425089105527
RMSE:
27.34
Testing kernel: rbf(0.1)
MSE:
830.86131562303
RMSE:
28.82

Layer: 10
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
823.1718286118793
RMSE:
28.69
Testing kernel: tribas
MSE:
716.0747134286931
RMSE:
26.76
Testing kernel: hardlim
MSE:
798.2842711099814
RMSE:
28.25
Testing kernel: rbf(0.1)
MSE:
828.8376115419017
RMSE:
28.79

Layer: 11
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
777.2621054753946
RMSE:
27.88
Testing kernel: tribas
MSE:
725.2093089430343
RMSE:
26.93
Testing kernel: hardlim
MSE:
753.6867092677923
RMSE:
27.45
Testing kernel: rbf(0.1)
MSE:
827.1201475614408
RMSE:
28.76

Layer: 12
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
815.2946943283509
RMSE:
28.55
Testing kernel: tribas
MSE:
745.2228517726139
RMSE:
27.30
Testing kernel: hardlim
MSE:
812.4312525989245
RMSE:
28.50
Testing kernel: rbf(0.1)
MSE:
825.139758615836
RMSE:
28.73

Layer: 13
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
774.6041782443305
RMSE:
27.83
Testing kernel: tribas
MSE:
733.1167290126325
RMSE:
27.08
Testing kernel: hardlim
MSE:
763.2652315891357
RMSE:
27.63
Testing kernel: rbf(0.1)
MSE:
810.3844754096511
RMSE:
28.47

Layer: 14
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
794.005293366794
RMSE:
28.18
Testing kernel: tribas
MSE:
732.3167813116693
RMSE:
27.06
Testing kernel: hardlim
MSE:
766.272599514871
RMSE:
27.68
Testing kernel: rbf(0.1)
MSE:
810.9794738403234
RMSE:
28.48

Layer: 15
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
810.4832689931374
RMSE:
28.47
Testing kernel: tribas
MSE:
752.0601859355501
RMSE:
27.42
Testing kernel: hardlim
MSE:
792.7033638108735
RMSE:
28.15
Testing kernel: rbf(0.1)
MSE:
811.507587250335
RMSE:
28.49

Layer: 16
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
828.8480888888074
RMSE:
28.79
Testing kernel: tribas
MSE:
707.2308324491589
RMSE:
26.59
Testing kernel: hardlim
MSE:
803.1245940277472
RMSE:
28.34
Testing kernel: rbf(0.1)
MSE:
828.944817575196
RMSE:
28.79

Layer: 17
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
770.3057329567688
RMSE:
27.75
Testing kernel: tribas
MSE:
740.8494096518574
RMSE:
27.22
Testing kernel: hardlim
MSE:
762.3975799693641
RMSE:
27.61
Testing kernel: rbf(0.1)
MSE:
859.2897464800288
RMSE:
29.31

Layer: 18
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
793.8213569883692
RMSE:
28.17
Testing kernel: tribas
MSE:
746.0738708192896
RMSE:
27.31
Testing kernel: hardlim
MSE:
781.9830176707825
RMSE:
27.96
Testing kernel: rbf(0.1)
MSE:
846.8039386753575
RMSE:
29.10

Layer: 19
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
825.1021140623114
RMSE:
28.72
Testing kernel: tribas
MSE:
764.2635595702848
RMSE:
27.65
Testing kernel: hardlim
MSE:
790.0336210120247
RMSE:
28.11
Testing kernel: rbf(0.1)
MSE:
875.626758169994
RMSE:
29.59

Layer: 20
Testing kernel: default
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tanh
MSE:
827.6855819487535
RMSE:
28.77
Testing kernel: tribas
MSE:
718.8422139042659
RMSE:
26.81
Testing kernel: hardlim
MSE:
829.1089287767466
RMSE:
28.79
Testing kernel: rbf(0.1)
MSE:
882.1224355303236
RMSE:
29.70
Optimal model:
ELMRegressor(hidden_layer=SimpleRandomHiddenLayer(activation_args=None,
                                                  activation_func='tanh',
                                                  n_hidden=1, random_state=0),
             regressor=None)
bestHiddenLayerCount:
1
MSE: 723.78
RMSE: 26.90
"Training LiPo Voltage prediction model"
Training using KDB+ input!
Using PCA!
Number of hidden layers: 1
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 2
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 3
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 4
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 5
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 6
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 7
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 8
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 9
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 10
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 11
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 12
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 13
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 14
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 15
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 16
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 17
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 18
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 19
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 20
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)


Testing all kernels...
testX
            0         1         2  ...         6         7         8
0    1.633114 -0.298498 -0.392264  ... -0.490881  0.030349  1.647568
1   -0.606720 -1.290262 -0.144270  ...  0.217441  1.312528 -0.326098
2   -0.678126 -3.688901  0.037087  ...  2.150481  1.715411 -0.883116
3   -0.738468 -4.123105  0.416833  ...  3.108261  1.757715 -0.400148
4   -0.371671 -4.246670  1.311340  ...  3.866260  1.058501 -0.261875
5   -0.664540 -3.905909  1.535097  ...  3.142465  0.613926 -0.463958
6   -0.613997 -3.992392  0.712230  ...  2.501942  0.341878 -0.445172
7   -0.559902 -2.285460  1.487800  ...  2.133175  0.049193 -1.101276
8   -0.583286 -2.061594  0.553462  ...  1.614122  0.107005 -0.612021
9   -1.668495  0.478121  0.230750  ...  0.616865 -0.416254 -1.385241
10  -1.632714  1.070112 -0.144677  ...  0.222474 -0.597112 -2.591906
11  -1.774003  0.993648 -0.088918  ...  0.018716 -0.256007 -1.723865
12  -1.894616  1.068103 -0.546934  ...  0.001014  0.035687 -1.336447
13  -1.967111  1.256010 -0.569326  ... -0.275416  0.234845 -1.413524
14  -2.153808  1.681153 -0.924758  ... -0.517273  0.327582 -1.447756
15  -2.363396  1.832134 -1.896711  ... -0.932500  0.415865 -1.445354
16  -2.558891  2.144118 -1.672869  ... -0.581477  0.338035 -1.685151
17  -2.629402  2.120106 -1.916083  ... -0.442615  0.336936 -1.373292
18  -2.593400  2.011825 -1.991513  ... -0.335507  0.414313 -1.367942
19  -2.134386  0.889034 -2.698127  ... -0.964147  0.657776 -1.330829
20  -0.968825 -0.823709 -0.661126  ...  1.178757  0.038661 -0.626487
21  -0.660809 -0.993669 -1.314497  ...  0.310047  0.081234 -0.713977
22  -0.441165 -0.817023 -0.946549  ...  0.369362 -0.046839 -0.400290
23   0.089854 -0.574002 -1.036962  ...  0.371061 -0.518728  0.235266
24   0.832047 -0.281925 -0.093209  ...  0.079165 -1.146520  0.359905
25   1.017300 -0.416205 -0.180270  ...  0.090698 -1.226486  0.806267
26   1.562205 -0.453180 -0.188378  ...  0.025425 -1.111221  1.388592
27   1.696992 -0.386823  0.021516  ... -0.026470 -0.978614  1.722322
28   1.882649 -0.719311 -0.504593  ... -0.921200 -0.480606  1.805269
29   1.730608 -0.606523  0.211119  ... -0.278916  0.119344  1.855430
30   1.793082 -0.563879 -0.355938  ... -0.599125 -0.551635  1.733158
31   1.638412 -0.256156 -0.516327  ... -0.361944 -0.653615  1.806792
32   1.509268 -0.031462 -0.413558  ... -0.056370  0.224459  2.146218
33   1.732516 -0.403503 -0.299523  ... -0.666457 -0.372209  1.694559
34   1.965227 -0.125209  0.142384  ... -0.479268 -1.084504  0.617384
35   1.734308 -0.156323 -0.065513  ... -0.224649 -0.679360  1.479106
36   0.962386 -0.187208 -0.020124  ...  0.015150 -0.570844  0.613165
37   0.167330 -0.479050  0.208671  ... -0.084864  0.152984 -0.300909
38  -0.499679 -0.402315  0.180386  ... -0.124809  0.566921 -0.919567
39   0.496283 -0.125366 -1.952206  ... -0.049703  1.711531  1.544823
40   0.865283 -0.016356 -1.859704  ...  0.207744  1.593421  2.215283
41   1.231005  0.460101 -0.782551  ...  0.143218  0.101213  1.851340
42   1.253295  0.353939 -0.640891  ...  0.193201 -0.024233  2.191322
43   1.616654  0.453288  1.164697  ...  0.010549 -0.588948  1.916677
44   1.782041  0.375273  0.811229  ... -0.583611 -1.720554  1.332547
45   1.916988 -0.455186  0.497879  ... -0.855674 -1.040479  1.484049
46   2.023453  0.021181  0.710576  ... -0.784088 -0.891101  0.523832
47   1.617959  0.229056  0.780138  ... -0.473940 -0.344678  1.624624
48   1.760421 -0.097565  0.727517  ... -0.675364 -0.417580  1.728294
49   1.718776  0.248949  0.727912  ... -0.635835 -1.277779  1.426025
50   1.810841 -0.069304 -0.085202  ... -1.294920 -0.791541  1.531281
51   1.474859 -0.427911 -0.141985  ... -0.623659  0.107033  1.556651
52  -0.868652 -2.324883  0.707787  ...  0.937132  2.134788 -0.759150
53  -0.967639 -3.577945 -0.430917  ...  1.866455  1.939794 -0.616360
54  -0.432057 -4.336930 -0.396361  ...  2.982498  1.692948 -0.385402
55  -2.455897  1.995158 -1.835194  ... -0.599702  0.137788 -1.984311
56  -2.593485  1.915046 -2.016567  ... -0.498864  0.401858 -1.376525
57  -2.114495  0.819656 -2.143207  ... -0.957595  0.545068 -1.362590
58  -1.533903 -0.467661 -2.498665  ... -1.864490  0.832112 -1.452661
59  -0.742994  0.574436 -2.451193  ... -1.089860  0.484199 -0.251875
60   1.428063  1.338856  0.296433  ... -0.746226 -0.193466  1.176244
61   1.652080  1.050749  0.267248  ... -0.767180 -0.145041  1.879243
62   1.958180  1.164980  0.879159  ... -0.702254 -0.791381  2.008480
63   2.145301  0.252952  0.564313  ... -1.448064 -1.020276  1.575891
64   1.962961  0.102122  0.410016  ... -0.596351 -1.021031  1.935011
65   2.019888  0.276229  1.210382  ... -0.361046 -2.275475  0.916509
66   2.034918 -0.253025  1.043161  ... -0.522420 -1.369719  1.592913
67   2.169741 -0.595894  1.410185  ... -0.345107 -1.949938  1.458510
68   2.103457 -0.435094  1.057018  ... -0.639783 -2.418779  1.190112
69   2.196691 -0.850217  0.243588  ... -1.078747 -2.144374  1.242697
70   2.136796 -0.890875  0.100797  ... -1.057094 -1.637505  0.807371
71   2.102534 -0.685273  0.240807  ... -0.822020 -1.765963  1.302078
72   1.873414 -0.001853 -0.755839  ... -0.983596 -1.943714  1.108544
73  -0.320694 -0.023308 -0.900909  ... -0.426592  0.169919 -0.295880
74  -0.720261 -0.706827 -0.497009  ... -0.205551  1.283073 -0.815588
75   1.911948 -0.612730 -0.826463  ... -0.661675 -0.473118  1.281377
76   1.844280 -0.376372 -0.418616  ... -0.342029 -0.164123  1.370455
77   1.708649 -0.414717 -0.929817  ... -0.474583 -0.124935  1.927128
78   1.581156 -0.241800 -0.604645  ... -0.270015  0.014594  2.000581
79   1.840847 -0.692376 -0.350595  ... -0.721068  0.232852  1.946396
80   1.772049 -0.585969 -0.999098  ... -0.382297 -0.375099  1.979207
81   0.539706 -0.310253  0.859806  ... -0.148040 -0.443642 -0.481641
82  -0.033596 -0.357677  0.708497  ... -0.020191 -0.012110 -0.357013
83  -0.750238 -0.407723  0.826653  ...  0.027086  0.487146 -1.043425
84  -0.904650 -0.177727  0.994660  ...  0.091843  0.271623 -1.238958
85  -0.916451 -0.310891  0.164019  ... -0.156060  0.336353 -1.178291
86  -1.153030  0.486288 -0.021677  ... -0.598877  0.105958 -2.098267
87  -1.490259  0.889465 -1.071306  ... -0.719648  0.297250 -1.662973
88  -0.860566 -1.139877 -3.046810  ... -0.999100  1.846671 -0.519058
89  -1.044034 -0.695455 -1.671819  ... -0.266661  2.026543 -0.902545
90   2.122021 -0.119536 -1.263761  ... -1.236246 -0.652680  1.854021
91   2.200975  0.058906  0.506028  ... -0.677690 -1.643481  1.018590
92   2.134057 -0.509960  0.865545  ... -0.514679 -1.186434  1.645975
93   2.213211 -0.526608  1.241562  ... -0.464898 -1.482611  1.562042
94   2.164617 -0.515286  1.460994  ... -0.452143 -2.497039  1.213565
95   2.062148 -0.493529  0.535761  ... -0.328809 -2.630508  1.331861
96   2.095556 -0.710483  0.691708  ... -0.773284 -1.912754  0.828702
97   2.066362 -0.656144  0.901005  ... -0.851059 -1.176484  1.479923
98   1.882855 -0.263379  0.775897  ... -0.791069 -1.404946  1.432703
99  -0.132196 -0.110476  0.473362  ... -0.502960 -0.108915 -0.177307
100 -0.784212 -0.835284 -0.059293  ... -0.399677  1.370932 -0.553707
101 -0.769569 -0.619155  0.603739  ... -0.011454  0.807120 -1.493590
102 -0.822545 -0.779133 -0.155253  ... -0.130802  1.036344 -0.889293
103 -0.823557 -0.716330 -0.368409  ... -0.033778  0.868354 -0.933076
104 -0.820056 -0.770701 -0.661045  ...  0.047150  0.710085 -0.934584
105  1.758620 -0.521254 -0.785576  ... -0.139094 -0.987683  1.671442
106  1.711937 -0.115364 -0.013222  ... -0.273370 -0.867743  1.705385
107  1.626103 -0.178295 -0.761908  ... -0.312644 -0.524799  2.011739
108  1.742678 -0.385005  0.615930  ... -0.285297 -0.141560  1.505127
109  1.604533 -0.243249 -0.018816  ... -0.358892 -0.684560  1.791663
110  1.584939 -0.458431  0.014516  ... -0.437594  0.497308  2.161541
111  1.552264 -0.186751  0.472407  ... -0.319147 -0.628295  1.785083
112  1.703817 -0.459162 -0.030540  ... -0.389467 -0.534623  2.000488
113  0.636366 -0.300583  1.345544  ... -0.115362 -0.583470 -0.252312
114  0.074396 -0.465166  1.353568  ... -0.082047  0.097857 -0.252693
115 -0.583265 -0.390598  1.202363  ... -0.122476  0.518403 -0.873014
116 -0.834722 -0.360650  0.865377  ...  0.035079  0.317921 -1.229974
117 -0.859659 -0.332366 -0.179287  ... -0.168058  0.356926 -1.159704
118 -0.982200  0.334572 -0.373968  ... -0.716173  0.178551 -2.065418
119  1.191955  0.338551 -1.814335  ...  0.165510  0.317808  2.318026
120  1.535673  0.637278  0.438397  ...  0.096089 -0.326895  1.595301
121  1.614431  0.325243  1.019274  ... -0.065460 -0.589667  1.893665
122  1.733191  0.507110  0.893709  ... -0.451426 -1.708159  1.418577
123  1.853453 -0.355640  0.605625  ... -0.775455 -1.046306  1.542363
124  1.870401 -0.215288  0.145596  ... -0.983067 -0.578219  1.807409
125  1.783403  0.250226  0.532680  ... -0.856326 -1.136733  1.056944
126  1.837005 -0.197262  0.927065  ... -1.000589 -1.053575  1.455707
127  1.797996 -0.143611  0.252257  ... -1.374238 -0.842300  1.500333
128  1.383083 -0.142732  0.487674  ... -0.437335 -0.132489  1.419594
129 -0.706903 -1.621470  0.417305  ...  0.408134  1.707161 -0.201875
130 -0.639614 -3.892696  0.842831  ...  2.427126  1.712635 -0.998347
131 -0.619181 -4.207774  0.276932  ...  3.042044  1.663071 -0.405202
132 -0.649266 -3.915099  1.073436  ...  3.823638  1.068093 -0.426233
133 -0.583031 -3.905156  0.617639  ...  3.048185  0.640173 -0.478485
134  0.834800 -0.345441 -0.590956  ...  0.214876 -1.063925  0.223519
135  0.979509 -0.442247 -0.670105  ...  0.000519 -1.000763  0.762570
136  1.521025 -0.634372 -0.310225  ... -0.097953 -0.460550  1.473837
137  1.755259 -0.538486 -0.325363  ... -0.174602 -1.462331  1.529590
138  1.644879 -0.278910 -0.124370  ... -0.221789 -0.627187  1.943836
139  1.722729 -0.350561 -0.007784  ... -0.580766 -0.680733  1.513424
140  1.825171 -0.810600  0.309727  ... -0.478897  0.439855  2.105313
141  1.754205 -0.496436  0.017869  ... -0.636609 -0.513200  1.771495
142  1.532558 -0.214398 -0.148377  ... -0.307613 -0.213421  1.959707
143  1.426272  0.048311 -0.335919  ...  0.063330 -0.723498  2.022440
144  1.667311 -0.278026  0.510066  ... -0.208760 -0.673370  1.503990
145  1.076894 -0.152025  0.907034  ...  0.154881 -0.381019  1.122059
146  0.190867 -0.338782  1.181936  ... -0.065904 -0.079960 -0.116578
147 -0.471573 -0.201662  1.381635  ... -0.166864  0.300557 -0.837507
148 -0.810059 -0.453201  0.998418  ... -0.001556  0.388391 -1.050083
149 -0.905256 -0.213602  1.302969  ... -0.152504  0.112943 -1.607246
150 -0.995740 -0.061680  0.344717  ... -0.575702  0.148880 -1.579569
151 -1.252297  0.486716 -0.225926  ... -0.699207  0.192263 -1.644790
152 -2.235108  2.143358 -1.027553  ... -0.614368  0.073515 -1.941975
153 -2.257277  2.768981 -1.509799  ... -1.004565  0.228078 -1.921096
154  0.406806 -0.111261  0.893076  ...  0.736593 -1.636109 -0.410860
155  0.580408 -0.512954  0.037701  ... -0.326704 -1.623585 -0.382962
156  0.230345 -0.591635  0.014144  ...  0.013457 -1.214422 -0.409110
157 -0.235932 -0.967949 -0.672919  ... -0.555103 -0.190684 -0.733481
158 -0.437631 -1.156006 -1.683476  ... -1.083379  0.539000 -0.716486
159 -0.841247 -1.091239 -1.487053  ... -1.061901  1.559679 -1.019683
160 -1.008982 -1.129223 -1.764591  ... -0.940874  2.045995 -0.623151
161 -1.933865  0.380808 -0.583118  ...  0.173133  0.795951 -1.749883
162 -1.919508  0.478412 -0.474417  ...  0.149974  0.853806 -1.716152
163 -1.736134  0.169289 -0.682076  ...  0.123364  0.858806 -1.452631
164 -1.518105  0.256423 -0.090767  ... -0.007176  0.812866 -1.820733
165 -1.499665  0.251160 -0.195481  ... -0.115570  0.808895 -1.462370
166 -1.436500  0.270716 -0.355597  ... -0.084293  0.685053 -1.435242
167 -1.480350  0.496244  0.125507  ...  0.587047  0.591950 -1.279445
168 -1.530018  0.426832 -0.231425  ...  0.720096  0.473353 -1.196278
169 -1.646507  0.089293 -1.848614  ...  0.146049  0.998978 -1.188576
170 -1.775230  0.238995 -0.513477  ...  0.173297  0.967781 -1.516798
171 -1.899576  0.820922 -0.749874  ...  0.017339  0.856016 -1.595529
172 -2.062966  1.007419 -0.721146  ...  0.018591  0.871239 -1.625316
173 -2.762938  1.714727 -1.212165  ...  0.267212  0.520987 -1.772299
174 -3.526038  3.720702 -0.097641  ...  1.234938 -0.071478 -2.324664
175 -3.653959  3.755263 -0.565727  ...  1.267035  0.068103 -1.825775
176 -4.013988  4.882052  0.278080  ...  2.393037 -0.108566 -1.486379
177 -4.128141  5.358022  0.940264  ...  3.145117 -0.353715 -1.278007
178 -4.027117  4.785112  0.323894  ...  3.268848 -0.394246 -1.000453
179 -2.982766  3.648531  1.503133  ...  3.212668 -0.441478 -1.162444
180 -2.625376  2.349308  0.785056  ...  2.540206  0.215958 -0.801116
181 -2.582154  1.905021  0.835966  ...  2.476362  0.459924 -0.932418
182 -3.105265  2.685663  1.235371  ...  3.263607  0.232475 -0.956871
183 -0.370508  0.231154 -0.424564  ... -0.463439  0.802079 -0.654199
184 -0.319264  0.144245 -0.295973  ... -0.484714  0.537547 -0.578211
185 -0.230914  0.049150 -0.133052  ... -0.374503  0.359553 -0.487094
186 -0.101431  0.047152 -0.522657  ... -0.307143  0.062285 -0.193527
187  0.099891  0.414389  0.241544  ... -0.461631 -0.092632 -0.604045
188  0.007368  0.420568 -0.327230  ... -0.246145  0.489304  0.166736
189  0.258314  0.428679 -0.186706  ... -0.496012 -0.068443  0.047193
190  0.266921  0.429677  0.048253  ... -0.425288 -0.259416 -0.115691
191 -0.264274  0.874430 -0.837323  ... -0.701832  0.394247 -0.317625
192 -1.057482  2.306789 -0.621770  ... -1.080241  0.275765 -1.319070
193  1.658071  1.048510 -0.536329  ... -0.731598 -0.371419  1.737355
194  1.155416  0.553331  0.351297  ... -0.564148 -1.061032  0.633610
195 -0.139092  0.513225  0.569832  ...  0.580538  0.572827  0.224665
196 -0.062344  0.317694  0.875797  ... -1.020228 -0.366228 -0.986941
197 -0.056655  0.325570  0.911629  ... -1.085181 -0.421427 -1.008918
198 -0.202636  0.392981  0.932558  ... -0.786510 -0.150602 -0.844722
199 -0.163093  0.453259 -0.120701  ... -0.518176 -0.133345 -0.357005
200 -0.070705  0.739614  0.462642  ... -0.989518 -0.312172 -0.627339
201 -0.319146  1.095625  0.528850  ... -0.338140  0.364303 -0.186954
202 -0.207772  0.861372  0.590434  ... -0.501120 -0.086496 -0.550942
203 -0.081954  0.020188 -1.007601  ... -0.644222  0.147603 -0.350986
204  1.596131  0.685507 -0.412440  ... -0.902483 -0.236534  1.334557
205  1.813689  0.694915 -0.130957  ... -0.816786 -0.146887  1.707152
206  1.769042  0.814235 -0.104845  ... -0.921039 -0.487549  1.583214
207  1.615175  0.941071 -0.376301  ... -0.605256 -0.176671  1.967156
208  1.768026  0.837574  0.336100  ... -0.912240 -0.686285  1.255241
209  1.613225  0.994787  0.149849  ... -0.634795 -0.464969  1.703059
210  1.773379  0.675034 -0.290640  ... -1.344750  0.298875  1.737223
211  1.556963  0.590786 -0.909097  ... -1.488727  0.665395  1.741311
212  1.256949  0.695133 -1.034583  ... -1.059795  1.230844  2.018685
213  1.230527  1.042453 -0.990834  ... -0.738275  1.022609  1.578287
214  1.313379  1.013928 -0.191495  ... -0.509276  0.981044  1.770272
215  1.335007  0.817990 -0.939564  ... -0.613929  1.053142  2.149286
216 -1.666207  0.269654 -1.060078  ...  0.094884  1.258422 -1.460754
217 -1.575233  0.398357  0.199126  ...  0.015157  1.041073 -2.024137
218 -1.663315  0.311199  0.052091  ... -0.090836  1.215844 -1.598431
219 -1.774694  0.580943  0.825034  ...  0.439587  1.049289 -1.550199
220 -1.794415  0.665963  0.997364  ...  0.378089  1.006558 -1.618664
221 -1.794136  0.644557  0.694536  ...  0.212848  0.956202 -1.553847
222 -1.029251  0.731742  1.040690  ... -0.453008  0.644666 -1.562241
223 -0.527656  0.998274  0.982814  ... -0.560345  0.219572 -0.989362
224 -0.018411  1.222711  1.014518  ... -0.789462 -0.158921 -0.680439
225 -0.003693  0.722837  0.690789  ... -1.059884 -0.430151 -0.894686
226  0.159261  0.351504  0.112068  ... -0.909244 -0.615163 -0.505766
227  0.600946  0.513301  0.055361  ... -0.829684 -0.930519 -0.520216
228  2.180638 -0.381794 -0.163688  ... -0.506224 -1.796495  1.379072
229  2.189048 -0.673126  0.338367  ... -0.243381 -2.292304  1.290114
230  2.127376 -0.517415  0.268979  ... -0.421038 -2.077480  1.288491
231  2.153567 -0.085754  0.881660  ... -0.744882 -2.612815  0.974062
232  2.254197 -0.641505  1.430122  ... -0.661552 -1.432642  0.767627
233  1.958241 -0.525535  0.510245  ... -0.423725 -1.015334  1.295951
234  1.918380 -0.521064  0.605740  ... -0.746301 -1.478417  1.421467
235  1.814489 -0.245166  0.181002  ... -1.001617 -1.326996  1.328459
236 -0.685659 -0.156123  1.072394  ... -0.386141  0.271983 -0.695764
237 -0.869938 -0.827827  0.432538  ... -0.245528  1.481260 -0.487592
238 -0.813987 -0.700317  1.291219  ... -0.111746  0.871238 -1.478413
239 -0.903984 -0.707175  1.037698  ... -0.158329  0.876708 -0.880092
240 -0.903176 -0.691120  0.887993  ... -0.121580  0.616633 -0.982955
241 -0.862824 -0.940864  0.571991  ...  0.101525  0.481377 -1.040909
242 -0.885508 -0.949094 -0.365641  ...  0.175089  0.599165 -0.827322
243 -0.743072 -1.493107  0.272537  ...  0.824828  0.525735 -1.487146
244  1.824639 -0.380601 -1.142869  ... -0.285161 -0.471172  1.817247
245  1.758053 -0.223312 -0.957366  ... -0.512217 -1.060634  1.596514
246  1.733611 -0.722273 -0.910719  ... -0.329271  0.338827  2.269294
247  1.890527 -0.656366 -0.209251  ... -0.797312 -0.159780  1.369804
248  1.600810 -0.279653 -0.162401  ... -0.413642  0.217215  2.077351
249  1.658982 -0.416026  0.251192  ... -0.317168  0.166667  1.998767
250  1.678879 -0.646418 -0.003993  ... -0.277657  0.211349  2.089207
251  1.581615 -0.021154 -0.337673  ... -0.094186 -0.684703  1.884387
252  0.363133 -0.334616  1.199874  ... -0.091162 -0.186902 -0.420768
253 -0.135918 -0.398639  0.874223  ... -0.051940  0.179972 -0.450254
254 -0.773194 -0.314751  0.497570  ...  0.078975  0.516953 -1.094496
255 -0.830972 -0.335583  0.266368  ... -0.047107  0.425955 -1.266748
256  1.550021  0.529806 -0.172036  ...  0.176861 -0.996150  1.435911
257  1.823548  0.245451 -0.241014  ... -0.269653 -1.523593  1.519134
258  1.923581  0.019611 -0.383808  ... -1.023579 -1.530456  1.291511
259  1.925313 -0.157086 -0.739271  ... -0.701685 -0.631079  1.654922
260  1.684322  0.161865 -0.574142  ... -0.491505 -0.226415  1.819299
261 -0.370050 -4.381251 -0.210001  ...  3.373204  1.307653 -0.450830
262 -0.933241 -2.884766  0.627865  ...  1.693089  0.866271 -1.323739
263 -0.947884 -2.877271  0.579681  ...  1.835757  0.675540 -0.881242
264 -0.893422 -2.641405  0.752495  ...  1.746634  0.731919 -0.779531
265 -0.797770 -2.244710  1.196731  ...  1.604470  0.714168 -0.791404
266 -0.848713 -2.073648  0.428998  ...  1.522819  0.743014 -0.530012
267 -0.694732 -1.373409  1.290604  ...  1.148907  0.451267 -1.366925
268 -0.738772 -1.167885  0.504419  ...  0.544850  0.348963 -1.019707
269 -0.709371 -0.590577 -0.096476  ...  0.278210  0.280427 -0.837582
270 -0.592320 -0.726414 -1.885509  ... -0.040023  0.326085 -0.563647
271 -0.830086  0.294931  1.462759  ...  1.415403 -0.558585 -1.051274
272 -0.860983  0.165049  0.497940  ...  1.272617 -0.324687 -0.683049
273 -1.488790  0.069426 -0.670806  ... -0.079731  0.695356 -1.442041
274 -1.735324 -0.028209 -1.206506  ... -0.414944  0.832494 -1.579341
275 -2.095197  0.241973 -0.511899  ... -0.425477  0.917597 -2.502168
276 -1.992805  0.473968 -1.055686  ... -0.548656  1.159600 -2.196888
277 -1.807644  0.139013 -0.787844  ... -0.598962  1.360165 -2.234744
278 -1.774878  0.486357 -0.003460  ...  0.022048  1.476424 -1.835011
279 -1.785451  0.325206 -0.440938  ... -0.161764  1.439175 -1.733020
280 -1.669400  1.265548  1.100833  ...  0.572720  0.881597 -2.069505
281 -1.490812  0.682291  0.932997  ...  0.463932  1.052359 -1.599380
282 -1.435331  0.088774  0.813833  ... -0.000998  1.026675 -1.802157
283 -1.582709 -0.338355 -1.020083  ...  0.006341  1.217412 -1.486192
284 -1.777143 -0.316895  0.112505  ...  0.007052  1.081660 -2.272169
285 -1.792470 -0.329310 -0.538312  ... -0.014847  1.135392 -1.957847
286 -4.274550  4.349676 -0.068027  ...  2.770405 -0.270427 -1.659651
287 -3.734092  4.976635  2.126268  ...  4.720190 -0.749141 -0.323632
288 -4.362101  4.952148  2.052668  ...  4.515463 -0.910630 -1.140017
289 -4.716533  5.321454  1.878959  ...  4.994073 -0.597120 -0.885829
290 -1.380740 -0.474719 -0.400171  ... -0.201055  1.272421 -1.564469
291 -1.668209  0.127828 -0.019207  ...  0.430035  1.320784 -1.108810
292 -1.586583  0.189263  0.001193  ...  0.412822  1.335551 -1.037449
293 -1.376672  0.394170 -0.141709  ...  0.219544  0.877343 -0.933462
294 -1.160620  0.160934 -0.340583  ...  0.097786  0.786274 -0.747574
295 -0.920914 -0.163767 -0.662936  ... -0.919792  0.779417 -1.473221
296 -0.898007 -0.176283 -1.034897  ... -0.989915  0.872957 -0.986060
297 -0.701992  0.470133  0.329259  ...  0.266726  0.573103 -0.336813
298 -0.057315  0.921396  0.603794  ...  0.479195 -0.270773  0.183967
299  0.598243  0.782643 -0.580988  ... -0.073616 -0.931745  0.516250
300 -0.554200  0.341786 -0.184942  ... -0.098271 -0.282070 -0.858197
301 -0.101956  0.347364 -0.729010  ... -0.148126 -0.484375 -0.210501
302 -1.545756  0.596221  0.403148  ...  0.180448  0.249992 -2.274272
303 -1.650064  0.457272  0.433015  ...  0.058269  0.453790 -1.836898
304 -1.681968  0.291860  0.273536  ... -0.052023  0.717493 -1.655799
305 -1.745904  0.365595  0.215047  ... -0.205601  0.851260 -1.727636
306 -1.918961  0.245442 -1.119332  ...  0.069094  1.030703 -1.626138
307  1.191955  0.338551 -1.814335  ...  0.165510  0.317808  2.318026
308  1.535673  0.637278  0.438397  ...  0.096089 -0.326895  1.595301
309  1.614431  0.325243  1.019274  ... -0.065460 -0.589667  1.893665
310  1.733191  0.507110  0.893709  ... -0.451426 -1.708159  1.418577
311  1.853453 -0.355640  0.605625  ... -0.775455 -1.046306  1.542363

[312 rows x 9 columns]
['default', 'tanh', 'tribas', 'hardlim', 'rbf(0.1)']

Layer: 1
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.052888708796488736
RMSE:
0.23
Testing kernel: tribas
MSE:
0.04821360361553578
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.054945822688956536
RMSE:
0.23
Testing kernel: rbf(0.1)
MSE:
0.050427150333381535
RMSE:
0.22

Layer: 2
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.06237608399228606
RMSE:
0.25
Testing kernel: tribas
MSE:
0.04231200873809767
RMSE:
0.21
Testing kernel: hardlim
MSE:
0.05851187695100981
RMSE:
0.24
Testing kernel: rbf(0.1)
MSE:
0.050219845020949096
RMSE:
0.22

Layer: 3
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.05327978958827641
RMSE:
0.23
Testing kernel: tribas
MSE:
0.041724899842070465
RMSE:
0.20
Testing kernel: hardlim
MSE:
0.05553985651952848
RMSE:
0.24
Testing kernel: rbf(0.1)
MSE:
0.04443516263920808
RMSE:
0.21

Layer: 4
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.05077797208156528
RMSE:
0.23
Testing kernel: tribas
MSE:
0.04906019325671497
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.052609833396339116
RMSE:
0.23
Testing kernel: rbf(0.1)
MSE:
0.048621799112718
RMSE:
0.22

Layer: 5
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.049174949953073095
RMSE:
0.22
Testing kernel: tribas
MSE:
0.04839405372006943
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.04842703680101991
RMSE:
0.22
Testing kernel: rbf(0.1)
MSE:
0.04989403569644576
RMSE:
0.22

Layer: 6
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.050963392437077745
RMSE:
0.23
Testing kernel: tribas
MSE:
0.04872382469750054
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.055268307467713704
RMSE:
0.24
Testing kernel: rbf(0.1)
MSE:
0.0522320762903955
RMSE:
0.23

Layer: 7
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.04894898249285374
RMSE:
0.22
Testing kernel: tribas
MSE:
0.05125575237436951
RMSE:
0.23
Testing kernel: hardlim
MSE:
0.05035187598501992
RMSE:
0.22
Testing kernel: rbf(0.1)
MSE:
0.05058891942484848
RMSE:
0.22

Layer: 8
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.05234693154897075
RMSE:
0.23
Testing kernel: tribas
MSE:
0.04459924189159388
RMSE:
0.21
Testing kernel: hardlim
MSE:
0.04932029704394757
RMSE:
0.22
Testing kernel: rbf(0.1)
MSE:
0.0507287141033054
RMSE:
0.23

Layer: 9
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.05709381388464306
RMSE:
0.24
Testing kernel: tribas
MSE:
0.0439802744082647
RMSE:
0.21
Testing kernel: hardlim
MSE:
0.05233434704942319
RMSE:
0.23
Testing kernel: rbf(0.1)
MSE:
0.050858787900918315
RMSE:
0.23

Layer: 10
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.05339445894530668
RMSE:
0.23
Testing kernel: tribas
MSE:
0.04408092463055812
RMSE:
0.21
Testing kernel: hardlim
MSE:
0.05475756417872325
RMSE:
0.23
Testing kernel: rbf(0.1)
MSE:
0.05067855256460105
RMSE:
0.23

Layer: 11
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.05481758088534182
RMSE:
0.23
Testing kernel: tribas
MSE:
0.0427813448950111
RMSE:
0.21
Testing kernel: hardlim
MSE:
0.05476622402419819
RMSE:
0.23
Testing kernel: rbf(0.1)
MSE:
0.05023351763930772
RMSE:
0.22

Layer: 12
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.04878218533177586
RMSE:
0.22
Testing kernel: tribas
MSE:
0.04641419738609217
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.048621816603530994
RMSE:
0.22
Testing kernel: rbf(0.1)
MSE:
0.050373709206440534
RMSE:
0.22

Layer: 13
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.04682815268188164
RMSE:
0.22
Testing kernel: tribas
MSE:
0.044469854228029945
RMSE:
0.21
Testing kernel: hardlim
MSE:
0.04973129567759307
RMSE:
0.22
Testing kernel: rbf(0.1)
MSE:
0.049768881817177445
RMSE:
0.22

Layer: 14
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.0471604779104088
RMSE:
0.22
Testing kernel: tribas
MSE:
0.04836123894300359
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.04780209825672993
RMSE:
0.22
Testing kernel: rbf(0.1)
MSE:
0.04974709872035239
RMSE:
0.22

Layer: 15
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.045034758312675345
RMSE:
0.21
Testing kernel: tribas
MSE:
0.047489182598526435
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.04986144518774512
RMSE:
0.22
Testing kernel: rbf(0.1)
MSE:
0.04908338354691553
RMSE:
0.22

Layer: 16
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.050637934177847926
RMSE:
0.23
Testing kernel: tribas
MSE:
0.05111839406508287
RMSE:
0.23
Testing kernel: hardlim
MSE:
0.04899611847396742
RMSE:
0.22
Testing kernel: rbf(0.1)
MSE:
0.050721520867619545
RMSE:
0.23

Layer: 17
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.05513953618168134
RMSE:
0.23
Testing kernel: tribas
MSE:
0.04989229671653551
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.052579911709153195
RMSE:
0.23
Testing kernel: rbf(0.1)
MSE:
0.05038013434096964
RMSE:
0.22

Layer: 18
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.0496139026616799
RMSE:
0.22
Testing kernel: tribas
MSE:
0.043206650282708786
RMSE:
0.21
Testing kernel: hardlim
MSE:
0.05124532442598476
RMSE:
0.23
Testing kernel: rbf(0.1)
MSE:
0.04855941991612898
RMSE:
0.22

Layer: 19
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.05030498330846889
RMSE:
0.22
Testing kernel: tribas
MSE:
0.052787116537743296
RMSE:
0.23
Testing kernel: hardlim
MSE:
0.05351724041216427
RMSE:
0.23
Testing kernel: rbf(0.1)
MSE:
0.04866142023031341
RMSE:
0.22

Layer: 20
Testing kernel: default
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tanh
MSE:
0.05114835226757841
RMSE:
0.23
Testing kernel: tribas
MSE:
0.04661294783950234
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.05288535158829593
RMSE:
0.23
Testing kernel: rbf(0.1)
MSE:
0.04874385417813282
RMSE:
0.22
Optimal model:
ELMRegressor(hidden_layer=SimpleRandomHiddenLayer(activation_args=None,
                                                  activation_func='tribas',
                                                  n_hidden=2, random_state=0),
             regressor=None)
bestHiddenLayerCount:
2
MSE: 0.04
RMSE: 0.21
"Deploying GPS speed prediction model"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
11046 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
"Deploying LiPo Voltage prediction model"
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
10470 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
"Parameters for data synthesis:"
"lowThrottle"
1000
"highThrottle"
2000
"throttleSteps"
10
"Synthesizing GPS speeds and LiPo voltage for different throttle values and t..
`LiPoPredictionTable
`gpsSpeedPredictionTable
"Generating timestep sample 2"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
76280 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
147848 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11407                           11407                      
1      22814                           22814                      
"Generating timestep sample 3"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
145539 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
410380 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11407                           11407                      
1      22814                           22814                      
2      34221                           34221                      
"Generating timestep sample 4"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
414775 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
561728 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11407                           11407                      
1      22814                           22814                      
2      34221                           34221                      
3      45628                           45628                      
"Generating timestep sample 5"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
544472 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
709537 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11407                           11407                      
1      22814                           22814                      
2      34221                           34221                      
3      45628                           45628                      
4      57035                           57035                      
"Generating timestep sample 6"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
701845 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
866551 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11407                           11407                      
1      22814                           22814                      
2      34221                           34221                      
3      45628                           45628                      
4      57035                           57035                      
5      68442                           68442                      
"Generating timestep sample 7"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2010, 1909, 1808, 1707, 1606, 1505, 1404, 1303, 1202, 1101, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
846490 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2010, 1909, 1808, 1707, 1606, 1505, 1404, 1303, 1202, 1101, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
981209 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11407                           11407                      
1      22814                           22814                      
2      34221                           34221                      
3      45628                           45628                      
4      57035                           57035                      
5      68442                           68442                      
6      79849                           79849                      
"Generating timestep sample 8"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2010, 1909, 1808, 1707, 1606, 1505, 1404, 1303, 1202, 1101, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
1017696 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2010, 1909, 1808, 1707, 1606, 1505, 1404, 1303, 1202, 1101, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
1128726 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11407                           11407                      
1      22814                           22814                      
2      34221                           34221                      
3      45628                           45628                      
4      57035                           57035                      
5      68442                           68442                      
6      79849                           79849                      
7      91256                           91256                      
"Generating timestep sample 9"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2010, 1909, 1808, 1707, 1606, 1505, 1404, 1303, 1202, 1101, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
1159086 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2010, 1909, 1808, 1707, 1606, 1505, 1404, 1303, 1202, 1101, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
1287611 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11407                           11407                      
1      22814                           22814                      
2      34221                           34221                      
3      45628                           45628                      
4      57035                           57035                      
5      68442                           68442                      
6      79849                           79849                      
7      91256                           91256                      
8      102663                          102663                     
"Generating timestep sample 10"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2010, 1909, 1808, 1707, 1606, 1505, 1404, 1303, 1202, 1101, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
1273019 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2010, 1909, 1808, 1707, 1606, 1505, 1404, 1303, 1202, 1101, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
1419970 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11407                           11407                      
1      22814                           22814                      
2      34221                           34221                      
3      45628                           45628                      
4      57035                           57035                      
5      68442                           68442                      
6      79849                           79849                      
7      91256                           91256                      
8      102663                          102663                     
9      114070                          114070                     
`:lookbackSteps.dat
"Saving synthesizedThrottleLSTMTrainingDataMatrix to disk"
"Saving realThrottleLSTMTrainingDataMatrix to disk"
"Training LSTM (Regression Window) using synthesized data!"
Training using KDB+ input!
Look back steps detected: 10
WARNING:tensorflow:From /home/foorx/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Epoch 1/100
 - 28s - loss: 0.0083
Epoch 2/100
 - 26s - loss: 0.0041
Epoch 3/100
 - 26s - loss: 0.0037
Epoch 4/100
 - 26s - loss: 0.0036
Epoch 5/100
 - 26s - loss: 0.0035
Epoch 6/100
 - 26s - loss: 0.0035
Epoch 7/100
 - 26s - loss: 0.0035
Epoch 8/100
 - 26s - loss: 0.0034
Epoch 9/100
 - 27s - loss: 0.0034
Epoch 10/100
 - 27s - loss: 0.0034
Epoch 11/100
 - 26s - loss: 0.0034
Epoch 12/100
 - 26s - loss: 0.0034
Epoch 13/100
 - 26s - loss: 0.0034
Epoch 14/100
 - 27s - loss: 0.0034
Epoch 15/100
 - 27s - loss: 0.0034
Epoch 16/100
 - 27s - loss: 0.0034
Epoch 17/100
 - 26s - loss: 0.0034
Epoch 18/100
 - 26s - loss: 0.0034
Epoch 19/100
 - 26s - loss: 0.0034
Epoch 20/100
 - 26s - loss: 0.0034
Epoch 21/100
 - 26s - loss: 0.0034
Epoch 22/100
 - 26s - loss: 0.0034
Epoch 23/100
 - 26s - loss: 0.0034
Epoch 24/100
 - 26s - loss: 0.0034
Epoch 25/100
 - 26s - loss: 0.0034
Epoch 26/100
 - 26s - loss: 0.0034
Epoch 27/100
 - 26s - loss: 0.0034
Epoch 28/100
 - 26s - loss: 0.0034
Epoch 29/100
 - 26s - loss: 0.0034
Epoch 30/100
 - 26s - loss: 0.0034
Epoch 31/100
 - 26s - loss: 0.0034
Epoch 32/100
 - 26s - loss: 0.0034
Epoch 33/100
 - 26s - loss: 0.0034
Epoch 34/100
 - 26s - loss: 0.0034
Epoch 35/100
 - 26s - loss: 0.0034
Epoch 36/100
 - 26s - loss: 0.0034
Epoch 37/100
 - 26s - loss: 0.0034
Epoch 38/100
 - 26s - loss: 0.0034
Epoch 39/100
 - 26s - loss: 0.0034
Epoch 40/100
 - 26s - loss: 0.0034
Epoch 41/100
 - 26s - loss: 0.0034
Epoch 42/100
 - 26s - loss: 0.0034
Epoch 43/100
 - 27s - loss: 0.0034
Epoch 44/100
 - 27s - loss: 0.0034
Epoch 45/100
 - 27s - loss: 0.0034
Epoch 46/100
 - 26s - loss: 0.0034
Epoch 47/100
 - 26s - loss: 0.0034
Epoch 48/100
 - 26s - loss: 0.0034
Epoch 49/100
 - 26s - loss: 0.0034
Epoch 50/100
 - 26s - loss: 0.0034
Epoch 51/100
 - 26s - loss: 0.0034
Epoch 52/100
 - 27s - loss: 0.0034
Epoch 53/100
 - 27s - loss: 0.0034
Epoch 54/100
 - 27s - loss: 0.0034
Epoch 55/100
 - 27s - loss: 0.0034
Epoch 56/100
 - 27s - loss: 0.0034
Epoch 57/100
 - 26s - loss: 0.0034
Epoch 58/100
 - 26s - loss: 0.0034
Epoch 59/100
 - 26s - loss: 0.0034
Epoch 60/100
 - 26s - loss: 0.0034
Epoch 61/100
 - 26s - loss: 0.0034
Epoch 62/100
 - 26s - loss: 0.0034
Epoch 63/100
 - 26s - loss: 0.0034
Epoch 64/100
 - 26s - loss: 0.0034
Epoch 65/100
 - 26s - loss: 0.0034
Epoch 66/100
 - 27s - loss: 0.0034
Epoch 67/100
 - 27s - loss: 0.0034
Epoch 68/100
 - 27s - loss: 0.0034
Epoch 69/100
 - 27s - loss: 0.0034
Epoch 70/100
 - 26s - loss: 0.0034
Epoch 71/100
 - 26s - loss: 0.0034
Epoch 72/100
 - 26s - loss: 0.0034
Epoch 73/100
 - 27s - loss: 0.0034
Epoch 74/100
 - 27s - loss: 0.0034
Epoch 75/100
 - 27s - loss: 0.0034
Epoch 76/100
 - 27s - loss: 0.0034
Epoch 77/100
 - 27s - loss: 0.0034
Epoch 78/100
 - 27s - loss: 0.0034
Epoch 79/100
 - 27s - loss: 0.0034
Epoch 80/100
 - 26s - loss: 0.0034
Epoch 81/100
 - 26s - loss: 0.0034
Epoch 82/100
 - 26s - loss: 0.0034
Epoch 83/100
 - 26s - loss: 0.0034
Epoch 84/100
 - 26s - loss: 0.0034
Epoch 85/100
 - 26s - loss: 0.0034
Epoch 86/100
 - 26s - loss: 0.0034
Epoch 87/100
 - 26s - loss: 0.0034
Epoch 88/100
 - 27s - loss: 0.0034
Epoch 89/100
 - 27s - loss: 0.0034
Epoch 90/100
 - 27s - loss: 0.0034
Epoch 91/100
 - 27s - loss: 0.0034
Epoch 92/100
 - 27s - loss: 0.0034
Epoch 93/100
 - 27s - loss: 0.0034
Epoch 94/100
 - 27s - loss: 0.0034
Epoch 95/100
 - 27s - loss: 0.0034
Epoch 96/100
 - 26s - loss: 0.0034
Epoch 97/100
 - 26s - loss: 0.0034
Epoch 98/100
 - 27s - loss: 0.0034
Epoch 99/100
 - 27s - loss: 0.0034
Epoch 100/100
 - 27s - loss: 0.0034
Finished training Regression (Normal) LSTM!
Using Regression (Normal) LSTM
Generating Rolling Launch Control Throttle sequence using KDB+ input!
Look back steps detected: 10
Rolling Launch Control Throttle generated!
"Transferring newly trained LSTM model to cloud!"
"Completed Updating Models"
16475615 13584255888
