Last login: Sun Mar 29 05:44:59 on ttys003
(base) Ren-Xiangs-MBP-2017:OHR400Dashboard foorx$ rlwrap q FASMLTrainingClientInit.q 
KDB+ 3.6 2019.08.20 Copyright (C) 1993-2019 Kx Systems
m64/ 4()core 16384MB foorx ren-xiangs-mbp-2017.local 127.0.0.1 EXPIRE 2020.10.02 foorenxiang@gmail.com KOD #5001568

Loading init.q
Loading util/init.q
Loading util/util.q
Loading util/metrics.q
Loading util/preproc.q
Loading fresh/init.q
Loading fresh/extract.q
Loading util/mproc.q
Loading fresh/select.q
Loading xval/init.q
Loading xval/xval.q
Loading util/mproc.q
Loading util/pickle.q
"Machine Learning toolkit loaded"
"Q ML Training Client Process running on port 6001 [websocket mode]"
KDB+ 3.6 2019.08.20 Copyright (C) 1993-2019 Kx Systems
m64/ 4()core 16384MB foorx ren-xiangs-mbp-2017.local 127.0.0.1 EXPIRE 2020.10.02 foorenxiang@gmail.com KOD #5001568

'-c
Using TensorFlow backend.
"Connected to kdb master in cloud!"
"Automatic ML model retraining enabled!"
"Rolling Launch Control Model Trainer Up and Ready"
q)\ts system"l FASUpdateModels.q"
"Not using train test split!"
"Training GPS speed prediction model"
Training using KDB+ input!
Using PCA!
principalComponents:
[[ 2.67925837e-02 -3.28086610e-02  4.63700579e-02 -3.82336795e-02
  -7.39877271e-02  9.71605280e-02  4.11428634e-01 -3.23045141e-01
  -3.83421248e-02 -6.02973019e-02  7.49600358e-02 -2.08921617e-01
   2.89475756e-02 -4.70302841e-02  3.88005008e-01  4.11058564e-01
   4.04831322e-01  4.06471826e-01]
 [-2.06437624e-03  6.66535410e-03  4.99240213e-02  4.52023517e-01
   1.43302202e-01 -3.75785386e-01  8.29103479e-02 -5.43845287e-02
   4.49450988e-01  1.26940673e-01 -4.04358230e-01  1.15992615e-01
  -4.12657188e-01  1.61286901e-01  1.48441304e-01  2.70492049e-02
   1.62303406e-02  8.45124271e-02]
 [-4.14100393e-01  4.08193950e-01 -1.39913593e-01  7.71285911e-02
  -5.19470260e-01 -9.00699651e-02 -2.64154458e-02  1.34725711e-02
   8.59521044e-02 -5.27102421e-01 -9.91400604e-02 -2.26643523e-01
  -7.70246160e-02 -6.92579900e-03 -6.86283688e-03 -3.34512303e-02
  -6.13311831e-02 -1.42679518e-02]
 [-5.68899897e-01  5.70993665e-01  4.92048361e-02 -6.25535599e-02
   3.70240660e-01  1.05698163e-01  4.47554897e-02 -5.34871472e-02
  -6.22112650e-02  3.39917260e-01  9.49035392e-02  2.13590005e-01
   5.12059956e-02  7.54638867e-02  2.49396884e-02  5.36643396e-02
   6.66358908e-02  2.38851606e-02]
 [-6.04907013e-02  7.61000211e-02  3.39281083e-01  2.39777774e-02
   6.76179987e-02 -1.99614398e-01 -2.93660549e-02  7.95499643e-02
   5.42863236e-03  1.88186406e-01 -1.26580692e-01 -3.65015714e-01
   2.62605639e-02 -7.91805286e-01 -2.69134804e-02 -7.93658364e-02
  -4.24843602e-02  4.55878764e-02]
 [ 7.80366651e-03  2.31566579e-02  8.90044139e-01 -4.70757640e-02
  -1.36803591e-01  6.49547067e-02  2.12496763e-02  1.86976584e-01
  -3.53847821e-02 -2.08610506e-01  2.71913901e-02  2.19063911e-01
  -3.02889865e-02  2.18129209e-01  2.91488270e-02  2.08782631e-03
  -8.38656126e-03  6.11647726e-02]
 [ 2.29521530e-03  1.22324332e-04  1.45966045e-02 -2.19111155e-01
  -5.90595438e-02 -5.19918570e-01 -2.01014141e-02 -3.15436146e-01
  -2.19647143e-01 -7.18167247e-02 -4.23832642e-01  3.14703046e-01
   4.50019530e-01  5.32444016e-03 -1.49457407e-01  5.60153170e-02
   1.33725267e-01 -9.29239384e-03]
 [-1.41024758e-02  2.29197543e-02  7.19117607e-02 -1.46980929e-01
   1.73636057e-01 -1.64612470e-01 -7.68060306e-03  2.37206346e-01
  -1.34969312e-01  1.44694512e-01 -2.27150651e-01 -7.01212735e-01
   1.35408798e-01  5.05993201e-01 -2.12957972e-02 -6.86500079e-03
   3.47180819e-02 -2.01975663e-02]
 [ 3.77094849e-03  1.27994101e-02 -2.23607353e-01 -4.83425654e-02
  -2.70597283e-02 -1.39769772e-01  1.79528853e-01  8.25604326e-01
  -6.35409257e-02 -3.36717698e-02 -7.14638507e-02  2.48238021e-01
   5.13273669e-02 -1.19605942e-01  5.07714770e-02  1.61997145e-01
   2.63268921e-01  1.62455890e-01]]
Number of hidden layers: 1
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 2
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 3
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 4
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 5
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 6
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 7
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 8
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 9
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 10
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 11
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 12
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 13
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 14
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 15
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 16
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 17
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 18
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 19
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 20
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)


Testing all kernels...
testX
            0         1         2         3         4         5         6         7         8
0    2.725993  0.327946  0.850788  1.733816  0.531039 -0.646005 -0.539915  0.456328  0.429088
1    2.706882  0.207881 -0.265617  0.114285 -0.747579 -0.270371 -0.460557  0.625338  0.400123
2    2.716898  0.679976 -0.147554  0.122156 -0.521828 -0.311360 -0.417297  0.386007  0.340610
3    2.409005  0.022343 -0.560875  0.588954  1.023385 -0.822938 -0.078326 -0.557331  0.596880
4   -1.568082 -1.501157 -1.317631 -1.388797  1.452167 -0.592688 -0.284858 -0.477093  0.880614
5   -2.304827 -4.572205 -0.648631  2.437849  1.762953 -0.613779  1.582692 -0.425996  1.158727
6   -2.355543 -5.141551 -1.451155  0.654892  0.241516 -0.002431  2.554273 -0.294493  1.270404
7   -1.977520 -6.163214 -0.375076  0.064354 -1.183801  0.676873  2.643198  0.001264  0.998245
8   -2.221072 -5.875630 -0.356252 -0.002327 -1.062154  0.600471  1.950984  0.528919  0.799941
9   -2.049115 -5.469459 -1.848972 -0.584459 -1.861106  0.577970  1.786087  1.028337  0.659751
10  -2.249601 -3.791002  0.711488  2.863712 -1.700967  0.805379  1.392200  1.203901  0.720872
11  -2.078219 -3.343241 -1.095132  0.627694 -2.094294  0.734239  0.926657  1.187847  0.608525
12  -2.394831  1.504969  1.027076 -1.028718  0.531699 -0.959662  0.070100  0.873309  1.238991
13  -2.756544  2.204202  3.383757  3.153400  0.919729 -0.940528 -0.260957  0.957096  1.239988
14  -2.635960  2.294873  1.460488  0.330187  0.443255 -0.945589 -0.472716  0.685162  1.221668
15  -2.644480  2.487981  0.327104 -0.999923  0.046015 -0.864317 -0.427789  0.408250  1.229097
16  -2.736906  2.750828  0.549259 -0.524363  0.031913 -0.836386 -0.686301  0.207950  1.289353
17  -2.911680  3.596168  0.167194 -0.544402 -0.282024 -0.817352 -0.717571  0.140663  1.234194
18  -3.066286  4.320264 -1.142922 -1.017927 -0.404699 -0.998199 -0.770840  0.209329  1.238805
19  -3.265975  4.750293  0.209808  1.316144 -0.692323 -0.783592 -0.311682  0.199352  1.465710
20  -3.192922  4.765442 -0.577939 -0.248907 -1.140644 -0.654151 -0.154680  0.214781  1.485362
21  -3.206990  4.625236 -0.359092  0.246050 -1.273423 -0.602953 -0.058255  0.114807  1.486848
22  -3.128915  3.508643 -2.040901  1.151069 -1.441047 -0.839979 -0.317467  0.432256  1.313415
23  -2.014430 -0.695922 -0.464053  0.083064 -1.329180 -0.016579  0.582944  0.689823  1.186372
24  -1.878419 -0.487453 -1.176844  0.583290 -1.149188 -0.309024 -0.181728  0.709589  1.076389
25  -1.430711 -0.324148 -1.054757  0.748417 -1.286815 -0.244946 -0.059787  0.650605  1.124293
26  -0.421316 -0.192242 -1.356200 -0.468428 -1.854767 -0.056571 -0.033699  0.806551  1.153143
27   0.772302  0.021051  0.370740  1.381111 -1.673936 -0.107142 -0.084195  1.020244  0.872200
28   1.155042 -0.045551 -0.457807  0.072589 -1.780355 -0.111901 -0.014020  1.070515  0.968108
29   2.168972 -0.014585 -0.607180  0.324031 -0.654500 -0.638311  0.234643  0.655362  0.799100
30   2.537611  0.095244 -0.399144  0.243173 -0.956562 -0.477141  0.322871  0.379595  0.785528
31   2.805631 -0.208653 -1.201001 -0.419678 -0.801227 -0.632542 -0.536972 -0.104580  0.638232
32   2.523840 -0.186615  0.333357  1.152395  0.592564 -0.797221  0.046498 -0.786429  1.009969
33   2.643623 -0.108490 -0.582172  0.560853 -0.572932 -0.620178 -0.256493 -0.053028  0.767559
34   2.487412  0.201457 -0.861094  0.310053 -0.466379 -0.660028 -0.031625  0.064172  0.843757
35   2.381884  0.426007 -0.597441  0.275473  0.246610 -0.743332  0.309564 -0.875466  1.018127
36   2.546575  0.056196 -0.218693  0.629230 -0.523981 -0.536113 -0.304270 -0.208525  0.759366
37   2.493715  0.138598  2.525965  3.843500 -0.607550 -0.283209 -0.158133  0.359899  0.631692
38   2.516431  0.046330  0.557160  1.092414 -0.514512 -0.462745 -0.070701 -0.043734  0.703420
39   1.063643 -0.364233  1.096792 -0.458745 -0.068358 -0.348671 -0.654065  0.116382  0.869443
40  -0.416975 -0.780852  1.261231 -1.166524  1.558048 -0.776827 -1.156614 -0.197344  0.779007
41  -1.505589 -0.788128  1.318476 -1.283520  1.788104 -0.779697 -1.412883 -0.371772  0.617164
42   0.414807  0.608633 -1.719816  1.691485 -0.986465  0.006543  0.326495 -1.571560  1.521295
43   1.300328  0.454669 -1.836390 -0.077143 -1.513607  0.249433  0.498213 -1.870141  1.395908
44   1.992339  0.666017  0.069137  1.925367 -2.177207  0.457706  0.412600 -0.770976  1.018959
45   2.276289  0.313314 -0.564169  0.125164 -1.598983  0.234655  0.232552 -0.838025  0.960405
46   2.725272  0.222775  1.152049 -0.807130 -1.282777  0.337225 -0.104182 -0.393617  0.737783
47   2.804417  0.303344  0.721258 -0.259177 -0.859407 -0.127954 -0.542599  0.915429  0.346359
48   2.853930 -0.136603 -0.014916  0.213375 -0.218380 -0.424406 -0.451046  0.458164  0.473376
49   2.599918  0.171804  2.686737  3.890690  1.613418 -0.784974 -0.402687  0.178955  0.566881
50   2.516872  0.548105  0.655260  1.166102 -0.274995 -0.218212 -0.011459 -0.302861  0.650469
51   2.710398  0.253677  0.123201  0.102139  0.017973 -0.375525 -0.216698 -0.179119  0.606956
52   2.677745  0.585677  0.130587  0.106047 -0.267436 -0.409812 -0.167885  0.691147  0.329809
53   2.810189  0.290343 -0.752548  0.393936  0.005792 -0.622840 -0.791175  0.205958  0.316514
54   2.200402 -0.101209 -1.030751 -0.869437  0.422562 -0.609813 -0.279073 -0.533392  0.483422
55  -2.290962 -2.853917  0.343785  0.996227  2.638176 -0.699472  0.321236 -1.086491  1.102722
56  -2.496955 -4.302980 -1.450590  0.415715  2.182935 -0.756261  1.255600 -0.643924  1.096336
57  -2.162578 -5.497652 -1.623368  0.806283  0.069318 -0.008605  2.312646 -0.240966  1.177019
58  -3.201157  4.548714  1.012212  2.023897 -0.852053 -0.693000 -0.327703  0.365079  1.389308
59  -3.175932  4.532361 -0.703699  0.167972 -1.329679 -0.666632 -0.220058  0.128053  1.436851
60  -3.197538  3.246598 -2.291369  1.317160 -1.368407 -0.893148 -0.441855  0.581230  1.324939
61  -3.241867  2.117462 -3.987939  2.503051 -1.021843 -1.312405 -1.041331  0.917704  1.178498
62  -1.965456  1.861251 -3.564958  0.433245 -2.221644 -0.641357 -1.488257  0.066926  1.072091
63   1.531617  1.236567  1.431013  2.282162 -2.842601  0.140074 -1.639010 -1.048414  0.607885
64   2.010810  0.774464 -0.027065  0.238363 -3.151784  0.169154 -1.616321 -1.038419  0.581757
65   2.676779  0.548993  0.645578 -0.237073 -2.972532  0.129693 -1.622302 -0.665986  0.297546
66   2.916370  0.269868 -0.062881  0.056047 -1.088866 -0.827470 -1.696356 -0.007765  0.127816
67   2.899975  0.260408 -0.653386 -1.485274 -1.334791 -0.658173 -0.670838  0.029709  0.315373
68   2.890587  0.214194  1.878072  1.745795 -0.934518 -0.771980 -0.505072  1.117502  0.030726
69   3.138903 -0.296788  0.667169 -0.479567 -1.040268 -0.735383 -0.692395  0.198053  0.127207
70   3.424159 -0.591237  1.038838 -1.152235 -0.073328 -1.073512 -0.400470  0.748181 -0.084873
71   3.345894 -0.357456  0.714179 -0.629538 -0.200701 -1.186899 -0.615488  1.285683 -0.238721
72   3.402923 -0.410449 -0.760678 -1.373910  0.318848 -1.607848 -0.747231  1.264916 -0.294708
73   3.033183 -0.350881  0.800225  2.337149  0.830615 -1.627850 -0.572903  0.820157 -0.049953
74   3.166599 -0.007978  0.201549 -0.482016 -0.840581 -0.953642 -0.294455  0.981697 -0.124782
75   2.915691  0.701999 -0.314319  0.526659 -1.126498 -1.065775 -0.448553  1.119100 -0.299991
76  -0.659885  0.791082 -0.253484 -0.562048  0.995480 -1.200835 -0.525051  0.153688  0.920323
77  -1.938781 -0.415907  0.327781 -0.259119  1.995169 -1.251531 -1.067251 -0.780120  0.963599
78   2.549762 -0.232237  0.736938  2.501846 -0.202514 -0.628168 -0.349661 -0.149896  0.800175
79   2.453353 -0.014873  1.350062  2.478630  0.127390 -0.566151 -0.035228 -0.489304  0.939683
80   2.532821  0.055616 -0.695508  0.186034 -0.274594 -0.640209 -0.124961 -0.458311  0.877416
81   2.388488  0.215362 -0.445044  0.294784 -0.895577 -0.311065  0.060897 -0.603658  1.002395
82   2.692364 -0.215743 -0.351798  0.128835  0.180545 -0.724316 -0.337589 -0.831359  0.837425
83   2.709956 -0.177451 -1.479196 -1.071154  0.072421 -0.854569 -0.061515 -0.259028  0.695338
84   0.108929 -0.657448  2.422732  0.962997  1.411317 -0.730939 -1.096723  0.168790  0.754489
85  -0.597263 -0.670754  0.977485 -1.375177  1.274379 -0.706136 -1.109623 -0.035641  0.712051
86  -1.782951 -0.826674  1.040222 -1.328350  1.901832 -0.861968 -1.313852 -0.273092  0.575610
87  -1.975212 -0.525474  1.121474 -1.453694  1.448918 -0.705629 -1.210673 -0.024556  0.389415
88  -1.964352 -0.415968  0.120121 -2.347928  1.139505 -0.719417 -1.282657  0.016380  0.296519
89  -2.443072  0.895447  1.905841  1.189004  1.367773 -0.786118 -1.443874  0.295801  0.296128
90  -2.497570  1.734790  0.252181 -1.035012  0.992167 -0.832168 -1.396440  0.113905  0.301709
91  -2.327422  0.255038 -4.496289  0.829011  0.272701 -0.685880 -0.336444 -0.240446  0.811382
92  -2.626067  0.105846 -1.407973  3.739337  0.187638 -0.196679 -0.101068 -0.835174  1.041823
93   2.937772  0.063523 -1.437967 -1.001848 -0.655236 -1.093304 -1.326423 -0.242522  0.275012
94   3.029462  0.016532  1.760500  1.797021 -0.398535 -0.973681 -0.802841  0.506733  0.039783
95   3.178787 -0.536198  0.826085 -0.607142 -0.752225 -0.771471 -0.679692  0.042879  0.160966
96   3.421034 -0.536936  1.086078 -1.106879  0.180462 -1.114368 -0.556371  0.291196  0.031796
97   3.385944 -0.458011  1.238424 -0.962577 -0.498524 -0.940326 -0.461656  1.364301 -0.164908
98   3.350583 -0.236189 -0.315519 -1.967629  0.529018 -1.561203 -0.140997  1.599351 -0.275434
99   3.053103 -0.169491  1.072189  2.142612  0.676266 -1.529858 -0.251879  1.045193 -0.112975
100  3.176488  0.002065  0.194984 -0.275427  0.298268 -1.324739 -0.332703  0.332093  0.016735
101  3.015446  0.432163  0.154913 -0.169614 -0.517906 -1.045517 -0.268769  0.511989 -0.065435
102 -0.230660  0.706893 -0.000505 -0.429702  1.105333 -1.269420 -0.507578  0.276461  0.835036
103 -1.810566 -0.456460 -0.997770 -1.921152  1.949643 -1.350973 -1.201887 -0.857197  0.999478
104 -2.206686 -0.549980  1.845339  1.956780  1.467363 -0.945300 -1.033188 -0.404467  0.906408
105 -2.078412 -0.592827  0.023055 -0.496218  1.023543 -0.958500 -1.104953 -0.530889  0.864371
106 -2.100689 -0.523788  0.107491 -0.528886  0.742034 -0.842075 -1.006397 -0.349057  0.840230
107 -2.122946 -0.615386 -0.134507 -0.644113  0.447117 -0.752860 -0.914381 -0.115379  0.867432
108  2.552108 -0.032920 -0.611446  0.403866 -1.057833 -0.500491  0.214450  0.431195  0.753208
109  2.574544  0.373144 -0.202573  0.096120 -0.847007 -0.487830  0.105738  0.265160  0.697319
110  2.563356  0.316168 -1.653035 -0.923152 -0.319303 -0.781995  0.043589 -0.076137  0.819447
111  2.441306 -0.027160  1.152327  2.161068  0.250769 -0.608317  0.004226 -0.545339  1.007647
112  2.480516  0.202407 -0.764522  0.443760 -0.450603 -0.658723 -0.031919  0.068375  0.844551
113  2.504621 -0.009571 -0.778660  0.405962  0.356752 -0.811222 -0.076403 -1.174184  0.999927
114  2.447179  0.272794 -0.351925  0.187640 -0.711997 -0.458375  0.047085  0.009407  0.780851
115  2.722351 -0.017287 -1.251140 -1.148320 -0.658532 -0.553554 -0.020569 -0.127381  0.687504
116  0.378138 -0.633521  2.279566  1.129915  0.901979 -0.577591 -1.001839  0.205506  0.838530
117 -0.409754 -0.781866  1.160573 -1.305939  1.541522 -0.778342 -1.156320 -0.201769  0.778097
118 -1.498188 -0.789167  1.215293 -1.426432  1.771165 -0.781248 -1.412581 -0.376308  0.616232
119 -1.903398 -0.772088  1.227046 -1.429013  1.512976 -0.720582 -1.312107 -0.091989  0.398534
120 -1.950848 -0.495860  0.034632 -2.607698  1.119676 -0.705381 -1.375162 -0.004231  0.323186
121 -2.367885  0.614925  1.819078  1.282351  1.292664 -0.789574 -1.684181  0.181571  0.192472
122  2.014625  0.699465 -2.077619 -0.273464 -2.042008  0.235008  0.501981 -0.858097  1.104276
123  2.461384  0.309545  1.885524  1.759790 -0.968750  0.253077 -0.047181 -0.726117  0.839461
124  2.720516  0.095723  1.037616 -0.718015 -1.318975  0.323844 -0.180376 -0.388103  0.717630
125  2.786810  0.421279  0.581821 -0.506273 -0.753681 -0.165497 -0.424532  0.882071  0.380375
126  2.818086 -0.032900 -0.182403  0.006617 -0.219726 -0.434825 -0.368577  0.456283  0.486565
127  2.884805  0.160497 -1.184768 -1.372445  0.901266 -0.884165 -0.544945  0.023327  0.517670
128  2.588999  0.498856  0.734213  2.368329  0.600985 -0.727134 -0.442318  0.495742  0.453249
129  2.831224  0.143420 -0.043077 -0.057624 -0.436441 -0.358689 -0.532624  0.463784  0.297590
130  2.821466  0.211177 -0.729434  0.404683 -0.236137 -0.539706 -0.871562  0.244624  0.284065
131  2.089361  0.150764 -0.461303  0.106385  0.551774 -0.633225 -0.103316 -0.333469  0.500424
132 -1.707185 -1.892854 -1.448730 -1.581863  1.886885 -0.663180 -0.160203 -0.874956  1.010455
133 -2.307013 -4.862863 -0.061829  2.889266  1.270573 -0.343454  1.873559 -0.423621  1.171987
134 -2.297423 -5.367586 -1.571679  0.656648  0.102936 -0.002387  2.365589 -0.248953  1.131229
135 -2.331007 -5.830397 -0.084226 -0.168219 -1.199397  0.736799  2.555594  0.068164  0.969089
136 -2.151284 -5.808357 -0.433939  0.073468 -1.139868  0.582475  1.933855  0.548280  0.758844
137  0.649141 -0.066638  0.699904  1.957203 -1.548002 -0.102450  0.014987  0.965479  0.950377
138  1.121587 -0.061027 -0.636996  0.178073 -1.575184 -0.268301 -0.061472  0.834524  0.791040
139  2.014173 -0.207035 -0.536804  0.146364 -0.309520 -0.635797  0.034634  0.069024  1.002874
140  2.565796 -0.064041 -0.746705  0.499292 -1.242841 -0.508744  0.155485  0.905742  0.701015
141  2.542396  0.223594 -0.972558 -0.706193 -0.596721 -0.578588  0.126932  0.040005  0.865618
142  2.522657  0.062348 -0.040493  1.651950 -0.376434 -0.678106 -0.255669  0.033616  0.772381
143  2.696715 -0.359808 -0.366767  0.057965  0.889621 -0.947432 -0.159978 -1.080186  1.055870
144  2.666565 -0.042676 -0.700206  0.426699 -0.525162 -0.659443 -0.273895 -0.123462  0.716220
145  2.408718  0.228746 -0.887303  0.478464  0.146253 -0.819828  0.018797 -0.417024  0.965645
146  2.391197  0.541557 -1.479519 -0.621390 -0.485686 -0.672040  0.460502  0.097625  0.816397
147  2.499259  0.066600  0.483680  1.653649  0.478157 -0.861768  0.119043 -0.045207  0.727678
148  1.523464 -0.292736  0.685464 -0.705372  0.175063 -0.474390 -0.363304 -0.284772  0.894078
149 -0.196260 -0.635889  1.011547 -1.190109  1.461717 -0.789648 -1.086447 -0.077340  0.811166
150 -1.292461 -0.583534  1.233389 -1.442113  1.858078 -0.857443 -1.415107 -0.210386  0.566225
151 -1.789614 -0.857919  0.594247 -2.310752  1.675643 -0.820767 -1.329325 -0.162053  0.417506
152 -2.085510 -0.471138  1.740468 -0.076170  1.393836 -0.699203 -1.392999  0.150983  0.319954
153 -2.179317  0.191499  0.680339 -0.701907  1.198311 -0.830330 -1.584839  0.219363  0.231526
154 -2.350239  1.010470  0.400305 -0.833206  1.088460 -0.836046 -1.503423  0.251783  0.248215
155 -2.901814  3.695694  0.837930 -0.888580  1.138130 -0.858746 -0.978306  0.207852  0.520537
156 -3.026006  4.290831  0.750348 -0.701271  0.144065 -0.507107 -1.532085 -0.095453  0.315793
157  0.196418 -1.036356  2.429670 -0.139793 -1.753449  0.776019 -0.322670  1.312638  0.307969
158  0.311497 -0.563478 -0.091673  0.183994 -0.975256 -0.037285 -0.577986  1.818358  0.161789
159 -0.157130 -0.585444 -0.282262 -0.033049 -0.920696  0.028324 -0.201799  1.542333  0.244591
160 -1.172532 -0.325062 -1.797821  0.915996  0.076011 -0.484114 -0.384906  1.158023  0.500387
161 -1.625143  0.025555 -3.532741  0.691867  0.486681 -0.816606 -0.537477  0.835498  0.580844
162 -2.420810  0.203472 -2.667212  3.330058  0.665707 -0.711812 -0.452283 -0.027120  0.828474
163 -2.521226  0.237417 -3.579159  2.162725  0.272618 -0.567419 -0.309439 -0.486382  0.925272
164 -2.376512  0.351608 -0.134453 -0.330700  0.333039  0.485719  0.597774 -0.182668 -1.717064
165 -2.404525  0.384098 -0.207791 -0.169490  0.266302  0.507040  0.453553 -0.327954 -1.751848
166 -2.221370 -0.090813 -1.174771 -1.374681  0.030875  0.560675  0.359183 -0.287492 -1.773656
167 -2.149371 -0.147780  0.676225  1.675913  0.194839  0.618733  0.138930 -0.416678 -1.741669
168 -1.974310 -0.067407 -0.343953 -0.101000 -0.123536  0.645811  0.059889 -0.388216 -1.790831
169 -1.883595 -0.077103 -0.323790 -0.105873 -0.143926  0.649193  0.069138 -0.296401 -1.779643
170 -1.770755 -0.414144  0.670558 -0.798207 -0.312259  0.968472  0.313827 -0.504018 -1.717753
171 -1.735330 -0.548445  0.059940 -2.198318 -0.348549  0.981623  0.427451 -0.334792 -1.704772
172 -2.013170 -0.206537 -2.400074 -2.485143 -0.240972  0.727242  0.521566 -0.355074 -1.740188
173 -2.222256 -0.019551 -0.457635 -0.067310  0.138673  0.741941  0.532403 -0.381894 -1.677012
174 -2.281059  0.854632 -0.310744 -0.189696  0.198550  0.679459  0.319903 -0.487639 -1.740667
175 -2.323177  1.196051 -0.286871 -0.222811  0.264024  0.652565  0.475192 -0.495894 -1.736493
176 -2.496886  2.717407 -0.595889 -1.892420  0.564876  0.468139  1.199129 -0.203287 -1.628739
177 -2.775988  5.024471  3.039950 -0.021402  0.987625  0.737384  2.165106 -0.210078 -1.318576
178 -2.677935  5.229794  1.610134 -1.692815  0.650451  0.738902  2.301023 -0.323167 -1.242004
179 -2.449856  5.882167  3.495795 -2.988446  0.164134  1.312856  2.889930 -0.887561 -1.036951
180 -2.165046  6.058562  4.650832 -3.539555 -0.184326  1.667054  3.465327 -0.981540 -0.883884
181 -2.000642  5.644445  3.441880 -4.720920 -0.220851  1.614742  3.785447 -0.792769 -0.784226
182 -1.521151  3.519793  5.329860 -1.030461 -0.438327  1.975829  3.428057 -0.601955 -0.703417
183 -1.569092  1.788797  2.828603 -2.088028 -0.884394  1.844627  2.613255 -0.838820 -1.055637
184 -1.819871  1.145223  2.592863 -1.953404 -0.618149  1.778194  2.505771 -0.811844 -1.080583
185 -1.920749  2.004389  3.847309 -2.891930 -0.448140  1.954086  3.225566 -0.915732 -0.915027
186 -0.966531 -0.642302 -0.205969  0.174718  0.702160  1.129629 -0.661442 -0.249225 -0.531557
187 -0.713174 -0.523650 -0.519895  0.071179  0.662221  1.048878 -0.383717  0.057735 -0.590640
188 -0.533144 -0.608084 -0.573416 -0.077983  0.561185  1.072014 -0.209758  0.215408 -0.591723
189 -0.175942 -0.528334 -1.411724 -1.056359  0.067803  1.158290  0.002331  0.474992 -0.594185
190 -0.076244 -0.257010  0.617109  1.735876  0.477073  1.178211 -0.148915  0.532880 -0.509069
191  0.072021 -0.142706 -0.817456  0.316054 -0.101386  1.331324  0.150542 -0.114181 -0.347493
192  0.400251 -0.119040 -0.643788  0.170085  0.015747  1.228785 -0.054983  0.422027 -0.508940
193  0.400971 -0.344553 -0.008239  0.239598  0.163099  1.256605 -0.202122  0.501824 -0.557069
194 -0.313934  0.387977 -0.831801 -1.627789  0.748719  1.017706 -0.683000 -0.185933 -0.748175
195 -1.002184  2.666160  1.357590  1.236501  1.294771  0.915646 -0.488971 -0.004848 -0.629476
196  3.221335  0.501034 -0.700908  0.298562  0.690677  0.969986  0.453581 -0.149509 -0.834824
197  2.202726 -0.274980  0.081388 -0.287211  0.157726  1.265879  0.097805  0.778397 -1.050438
198  0.295666 -1.317008  0.181504 -4.585379  1.608174  1.380143 -0.126664 -0.894574 -0.736033
199 -0.157987 -0.404196  0.075685 -0.356376  1.717537  0.790268 -0.710815  0.870342 -1.284057
200 -0.178489 -0.351683 -0.058580 -0.477875  1.707838  0.792267 -0.736421  0.993585 -1.266250
201 -0.240044 -0.411805  0.115382 -0.342103  1.288297  1.027214 -0.522241  0.595323 -1.176214
202  0.016609 -0.474114 -1.545107 -3.275505  0.587987  1.254925 -0.375906  0.463169 -1.102878
203  0.000578  0.151260 -0.371122 -0.123705  0.676694  1.147492 -0.600842  0.675808 -1.130675
204 -0.121294  0.219985  0.081788 -0.370468  0.100648  1.616738 -0.199974 -0.249523 -0.827446
205 -0.154571 -0.222936  0.284441 -0.574788  0.416371  1.498796 -0.450319  0.371770 -1.000341
206 -0.079271 -0.758647 -2.183466 -2.582632  0.408914  1.254428 -0.194032  0.512325 -1.104119
207  2.844248  0.024129 -0.718153  0.554280  0.257605  1.462865  0.275277  0.000591 -1.000233
208  3.322355  0.054280 -0.691853  0.312596  0.579127  1.359295  0.497879 -0.256331 -1.039512
209  3.276718  0.162100 -0.704889  0.678963  0.207507  1.444424  0.372599  0.098710 -1.029008
210  3.233325  0.311727 -1.512825 -0.809093  0.227465  1.491518  0.700925 -0.227858 -0.886592
211  3.243591  0.088359  0.326766  1.808056  0.167808  1.568879  0.343308  0.231534 -1.048715
212  3.187912  0.339644 -0.660666  0.341437  0.028250  1.602433  0.678050  0.054856 -0.946040
213  3.233995  0.447383 -1.428052  0.739531  1.577990  0.911988  0.341745 -0.459079 -0.934638
214  2.870280  0.665838 -2.155947  1.820733  1.329360  0.917561  0.457451 -0.613741 -0.779571
215  2.574844  0.750501 -2.227072  1.961672  0.577667  1.315097  0.870874 -1.218501 -0.570541
216  2.448501  0.630468 -0.417261  3.764417  0.023351  1.757608  0.833593 -1.267435 -0.496286
217  2.681158  0.407249  0.186810  2.527562 -0.240874  2.008645  0.894961 -1.373627 -0.570550
218  2.821104  0.292397 -1.221354  0.797537 -0.230502  1.860391  0.821250 -1.375497 -0.612679
219 -2.502888 -0.825788 -1.605807 -1.155516  0.526537  1.853292  0.201915 -0.191572 -1.534020
220 -2.538315 -0.838145  0.638949  1.778990  0.808246  1.927932  0.052575 -0.112926 -1.519043
221 -2.475284 -0.817064 -0.813876  0.095333  0.652934  1.852130  0.007460 -0.227617 -1.530678
222 -2.443137 -0.948819  0.052945 -0.723302  0.588932  2.088420  0.193177 -0.320687 -1.545795
223 -2.493382 -0.859714  0.140114 -0.590899  0.669744  2.079876  0.126260 -0.252027 -1.530048
224 -2.447864 -0.716368 -0.725596 -1.641009  0.635827  2.009161  0.102997 -0.092573 -1.583961
225 -1.681211 -0.510258  0.659379  1.285124  0.603984  2.040215 -0.471346 -0.001729 -1.444112
226 -1.006773 -0.642020  0.050107 -0.367925 -0.155661  2.271100 -1.009551  0.079036 -1.587421
227 -0.221977 -0.643438  0.507228 -0.618654 -0.544825  2.395859 -1.523457 -0.033780 -1.758037
228 -0.145946 -0.667421 -0.311475  0.224270 -0.059487  2.017642 -0.916190  0.866862 -1.739553
229  0.365274 -0.684676 -1.382562 -1.142998 -0.082682  1.958104 -0.152240  1.334912 -1.542517
230  0.934648 -0.422479  0.294393  1.665505  0.265106  1.911323  0.053603  1.392033 -1.300118
231  3.193210 -0.434515  0.840294 -0.076094 -0.905580 -0.833538 -0.671794  0.663776 -0.001013
232  3.312774 -0.664504  1.087443 -0.883535 -0.225339 -1.040965 -0.333184  1.178723 -0.062702
233  3.280142 -0.410529  0.757686 -0.736689  0.465358 -1.374559 -0.397703  1.003837 -0.101897
234  3.286235  0.344124  0.748687 -0.294258 -0.527670 -1.183799 -0.370534  1.714290 -0.394714
235  3.143669 -0.161322  2.530972  2.397900  0.511422 -1.168466 -0.156096  0.495006 -0.019272
236  2.927945  0.051535  0.783309  1.751085  1.149591 -1.599772  0.073493  0.124399  0.206156
237  3.036624  0.149663 -0.158584 -0.003435 -0.105674 -1.271248 -0.233313  0.619896 -0.018267
238  2.916631  0.460875 -0.661360  0.341042  0.004453 -1.428517 -0.480619  0.469573 -0.098677
239 -1.192550  0.569462  0.346961 -0.478588  1.750323 -1.339334 -0.649846  0.112371  0.904584
240 -1.882288 -0.516037 -0.917601 -1.858361  1.638965 -1.202894 -1.097249 -1.032899  0.980056
241 -2.195457 -0.634712  1.839481  1.986274  1.379308 -0.920895 -1.141219 -0.514319  0.901435
242 -2.063031 -0.536855  0.009945 -0.691906  0.951627 -0.925439 -1.133614 -0.425464  0.836111
243 -2.096089 -0.538960 -0.001618 -0.459579  0.755190 -0.884436 -1.107280 -0.132094  0.829004
244 -2.100847 -0.866388  0.216893 -0.049851  0.464478 -0.757926 -0.901185  0.024521  0.851992
245 -2.072445 -0.932473 -0.725552 -1.712224  0.204205 -0.698887 -0.895432 -0.079279  0.820599
246 -2.336781 -2.035481  1.838719  1.833030  0.431395 -0.446284 -0.387434  0.099171  0.979213
247  2.587300  0.113750 -0.677232  0.352554 -0.118885 -0.794609  0.049842 -0.074686  0.898808
248  2.549084  0.265688 -0.721431  0.438262 -0.772771 -0.628922 -0.168204  0.522217  0.719668
249  2.637706 -0.223953 -1.385379 -1.003818  0.513468 -0.901876  0.024559 -0.945582  1.014178
250  2.581076 -0.288255  0.684361  2.497231  0.117113 -0.721871 -0.488365 -0.500392  0.841164
251  2.495889  0.182356 -0.594877  0.173685 -0.617359 -0.452912 -0.048370 -0.862792  0.927925
252  2.510633  0.020364 -0.055189  0.482558 -0.055249 -0.560430  0.024321 -0.814722  0.992031
253  2.578953 -0.197413 -0.531883  0.221762  0.405067 -0.794393  0.088171 -0.868439  0.957506
254  2.538165  0.245762 -1.137186 -1.090820 -0.636286 -0.557924  0.086344 -0.022075  0.696905
255 -0.114575 -0.692688  2.207366  0.668121  1.683190 -0.781576 -1.101280 -0.047377  0.856254
256 -0.796592 -0.734486  1.080402 -1.373802  1.527446 -0.749951 -1.207362 -0.176207  0.764405
257 -1.844038 -0.769446  1.133965 -1.401188  1.935494 -0.858203 -1.302707 -0.296132  0.540458
258 -1.970037 -0.646415  1.136641 -1.360219  1.411164 -0.700022 -1.338259 -0.135428  0.384850
259  2.452995  0.198619  1.783580  1.460488 -1.459393  0.367785  0.007445 -0.001040  0.731405
260  2.880804  0.042274  0.684404 -0.514045 -1.169405  0.017439 -0.337811  0.610062  0.356562
261  2.836252  0.181258  0.225067 -0.199588 -1.087154 -0.125087 -0.774225  0.912747  0.269669
262  2.803092  0.175270 -0.261353  0.007837  0.311838 -0.623037 -0.287065  0.077578  0.530067
263  2.569041  0.540228 -0.058862  0.175509 -0.394668 -0.233515 -0.009724 -0.335588  0.641574
264 -2.156241 -5.766272 -1.214269  0.866150 -0.274246  0.196141  2.567609  0.139647  1.199318
265 -2.068624 -3.725809 -0.300792  2.475696 -0.339845 -0.146407  1.912509  0.185146 -1.077431
266 -1.853111 -3.799921 -1.305129  0.712428 -0.987932  0.021534  1.969318  0.298655 -1.078494
267 -1.735714 -3.618513 -1.078498  0.600402 -1.304152  0.153443  1.755604  0.062710 -1.125443
268 -1.712787 -3.357259 -0.669752  0.543608 -1.342243  0.254095  1.474866  0.016030 -1.222870
269 -1.670081 -3.107697 -1.920613 -0.775129 -1.417432  0.217379  1.401844 -0.010986 -1.147556
270 -1.722776 -2.391654  1.159940  2.801189 -1.030188  0.339410  0.892426 -0.015808 -1.199748
271 -1.584818 -1.923793 -0.606084  0.781699 -1.043748  0.030992  0.182569 -0.015675 -1.387466
272 -1.358730 -0.974440 -0.728662  0.347023 -1.204037  0.005914 -0.175690 -0.359211 -1.471982
273 -1.061842 -0.975091 -3.277014 -3.137694 -1.593311 -0.064737 -0.427124 -0.372853 -1.548992
274 -0.906651 -0.698853  2.603284  0.473379 -0.985702  0.487314  0.738186  0.021911 -1.269522
275 -0.803110 -0.701478  1.497157 -1.228071 -1.305539  0.503223  0.562407 -0.245784 -1.294620
276 -1.845049  0.192812 -0.593526  0.040564  0.256033  0.163872  0.452163 -0.144943 -1.636794
277 -2.163665  0.486746 -1.989807 -0.845220  0.504373 -0.138645  0.343813 -0.079130 -1.793468
278 -2.953604  0.772877 -0.261432  2.377411  1.149729 -0.262692  0.350277 -0.027658 -1.895022
279 -2.883047  1.203225 -2.030424  0.920864  1.083513 -0.444222  0.332123 -0.071057 -1.884500
280 -2.900069  0.667282 -1.835589  0.829278  0.725388 -0.223091  0.048967 -0.300409 -1.968247
281 -2.629949  0.235695 -0.443905 -0.082320  0.281209  0.292204 -0.082682 -1.043237 -1.898734
282 -2.573272  0.093374 -1.308001 -1.276848  0.059032  0.290839 -0.260577 -0.984505 -1.991507
283 -2.354671  0.359973  2.400895  0.650347  0.055156  0.798577 -0.185807 -1.104192 -1.904332
284 -2.159770 -0.358913  0.636900 -0.775791 -0.252522  0.753421 -0.114524 -0.959399 -2.017680
285 -2.226688 -0.814150  0.186479 -0.488250 -0.092096  0.586311 -0.376288 -0.647525 -2.057969
286 -2.274850 -1.072465 -2.558990 -3.854446 -0.335261  0.516797 -0.237655 -0.695052 -2.034820
287 -2.756138 -0.870333  0.722861  1.594420  0.459426  0.380433 -0.072233 -0.532591 -1.981952
288 -2.652003 -0.767566 -0.220105  0.286771  0.341665  0.325020 -0.024263 -0.528743 -1.968079
289 -2.669713  5.853333  2.953018 -2.684620  0.725944  1.094208  4.052184 -0.138564 -0.841927
290 -1.172505  4.589567  5.937231 -4.864301 -0.855991  2.468111  4.687397 -0.992695 -0.440176
291 -2.104838  5.327807  5.505865 -4.097170  0.063460  1.964458  5.065680 -0.227571 -0.475337
292 -2.247876  5.499919  5.187450 -6.205422 -0.016974  2.102399  5.235313 -0.812075 -0.448632
293 -2.676401 -0.687641 -1.184233  2.987389  0.025168  1.089176  0.306552  0.229088 -0.308387
294 -2.578848 -0.493449 -0.769816  0.185689 -0.403895  1.456765  0.271772 -0.427443 -0.351373
295 -2.462380 -0.444592 -0.771436  0.183769 -0.217866  1.388251  0.253596 -0.491896 -0.376656
296 -1.987039 -0.013395 -0.712694  0.166668 -0.363032  1.355488  0.213965 -0.193974 -0.401083
297 -1.751840 -0.365271 -1.526871 -1.059146 -0.489258  1.365316  0.072013 -0.013671 -0.401684
298 -1.940547  0.022649 -1.254383  3.226530  0.158055  0.874374 -0.191608  0.502729 -0.444194
299 -1.731456  0.161656 -2.668242  1.521335 -0.005664  0.788067 -0.125251  0.455592 -0.516364
300 -0.988031 -0.278035 -0.296427  0.281727 -0.754615  1.614241  0.257085 -0.133355 -0.294605
301  0.238625 -0.241536  0.565562 -0.524124 -1.450399  1.935732  0.293405  0.156802 -0.456470
302  1.286113 -0.120397 -0.815858 -1.561518 -1.606828  1.620026  0.169444  0.736730 -0.920133
303 -0.509885 -0.227375  0.250353 -0.199933 -0.688535  0.669903 -0.081473  0.194906 -1.634477
304  0.307271 -0.162353 -1.364057 -2.667580 -0.559709  0.533156  0.032769  0.322851 -1.554064
305 -2.090722  0.371015  1.851522  2.579144  0.727959  0.378906  0.459217  0.066269 -1.672537
306 -2.087140  0.318132  0.563754  0.882250  0.507606  0.332746  0.344864 -0.060616 -1.756703
307 -2.165857  0.076601 -0.236885 -0.240999  0.338960  0.347210  0.109537 -0.306654 -1.865215
308 -2.281142  0.194085 -0.166483 -0.291092  0.309157  0.372771 -0.126210 -0.482185 -1.885805
309 -2.575086 -0.087225 -2.082409 -2.363694  0.242512  0.303078  0.158196 -0.393914 -1.985484
['default', 'tanh', 'tribas', 'hardlim', 'rbf(0.1)']

Layer: 1
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
721.5815168364229
RMSE:
26.86
Testing kernel: tribas
MSE:
720.5881297600337
RMSE:
26.84
Testing kernel: hardlim
MSE:
718.9094061185598
RMSE:
26.81
Testing kernel: rbf(0.1)
MSE:
730.8355273372567
RMSE:
27.03

Layer: 2
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
732.179603980834
RMSE:
27.06
Testing kernel: tribas
MSE:
716.5957832501645
RMSE:
26.77
Testing kernel: hardlim
MSE:
733.2367315990227
RMSE:
27.08
Testing kernel: rbf(0.1)
MSE:
725.875079370166
RMSE:
26.94

Layer: 3
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
749.6801238513837
RMSE:
27.38
Testing kernel: tribas
MSE:
719.0147755792717
RMSE:
26.81
Testing kernel: hardlim
MSE:
726.992914468217
RMSE:
26.96
Testing kernel: rbf(0.1)
MSE:
710.6361693641057
RMSE:
26.66

Layer: 4
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
716.8705933936626
RMSE:
26.77
Testing kernel: tribas
MSE:
722.3020920459758
RMSE:
26.88
Testing kernel: hardlim
MSE:
725.3479905874909
RMSE:
26.93
Testing kernel: rbf(0.1)
MSE:
719.5300499153107
RMSE:
26.82

Layer: 5
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
724.530319684262
RMSE:
26.92
Testing kernel: tribas
MSE:
717.7780146622059
RMSE:
26.79
Testing kernel: hardlim
MSE:
729.3909852481689
RMSE:
27.01
Testing kernel: rbf(0.1)
MSE:
829.3727898636672
RMSE:
28.80

Layer: 6
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
747.2206650943274
RMSE:
27.34
Testing kernel: tribas
MSE:
729.0953142029117
RMSE:
27.00
Testing kernel: hardlim
MSE:
743.335355568525
RMSE:
27.26
Testing kernel: rbf(0.1)
MSE:
826.336090348858
RMSE:
28.75

Layer: 7
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
764.4219838815576
RMSE:
27.65
Testing kernel: tribas
MSE:
730.0337370380618
RMSE:
27.02
Testing kernel: hardlim
MSE:
749.1790042997438
RMSE:
27.37
Testing kernel: rbf(0.1)
MSE:
822.8868249594792
RMSE:
28.69

Layer: 8
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
778.8261378801948
RMSE:
27.91
Testing kernel: tribas
MSE:
715.633836770064
RMSE:
26.75
Testing kernel: hardlim
MSE:
756.6239695982425
RMSE:
27.51
Testing kernel: rbf(0.1)
MSE:
859.9562975603711
RMSE:
29.33

Layer: 9
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
741.1611803096916
RMSE:
27.22
Testing kernel: tribas
MSE:
721.7485128502357
RMSE:
26.87
Testing kernel: hardlim
MSE:
714.9369386420854
RMSE:
26.74
Testing kernel: rbf(0.1)
MSE:
842.0516184992255
RMSE:
29.02

Layer: 10
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
836.7872917082981
RMSE:
28.93
Testing kernel: tribas
MSE:
730.7698547205257
RMSE:
27.03
Testing kernel: hardlim
MSE:
819.4126702669439
RMSE:
28.63
Testing kernel: rbf(0.1)
MSE:
848.5769440867927
RMSE:
29.13

Layer: 11
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
767.7633884816911
RMSE:
27.71
Testing kernel: tribas
MSE:
724.2948049583784
RMSE:
26.91
Testing kernel: hardlim
MSE:
751.0036359272614
RMSE:
27.40
Testing kernel: rbf(0.1)
MSE:
851.6412399756767
RMSE:
29.18

Layer: 12
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
803.741492687758
RMSE:
28.35
Testing kernel: tribas
MSE:
740.3269138252676
RMSE:
27.21
Testing kernel: hardlim
MSE:
788.921059248575
RMSE:
28.09
Testing kernel: rbf(0.1)
MSE:
858.4479827048683
RMSE:
29.30

Layer: 13
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
750.4740332801576
RMSE:
27.39
Testing kernel: tribas
MSE:
719.5232842321838
RMSE:
26.82
Testing kernel: hardlim
MSE:
762.3141916662297
RMSE:
27.61
Testing kernel: rbf(0.1)
MSE:
857.2715583032121
RMSE:
29.28

Layer: 14
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
791.0025606348
RMSE:
28.12
Testing kernel: tribas
MSE:
721.5235421034905
RMSE:
26.86
Testing kernel: hardlim
MSE:
752.5978004994707
RMSE:
27.43
Testing kernel: rbf(0.1)
MSE:
864.3622214686959
RMSE:
29.40

Layer: 15
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
798.2082231373126
RMSE:
28.25
Testing kernel: tribas
MSE:
736.4916480294714
RMSE:
27.14
Testing kernel: hardlim
MSE:
776.5938089989603
RMSE:
27.87
Testing kernel: rbf(0.1)
MSE:
840.7232064087111
RMSE:
29.00

Layer: 16
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
841.9759594238018
RMSE:
29.02
Testing kernel: tribas
MSE:
727.9184338313995
RMSE:
26.98
Testing kernel: hardlim
MSE:
795.3042500626258
RMSE:
28.20
Testing kernel: rbf(0.1)
MSE:
843.7428578078403
RMSE:
29.05

Layer: 17
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
766.2176914140744
RMSE:
27.68
Testing kernel: tribas
MSE:
736.0095873206319
RMSE:
27.13
Testing kernel: hardlim
MSE:
744.0247636549393
RMSE:
27.28
Testing kernel: rbf(0.1)
MSE:
848.515462267473
RMSE:
29.13

Layer: 18
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
798.732426393081
RMSE:
28.26
Testing kernel: tribas
MSE:
751.6092389654419
RMSE:
27.42
Testing kernel: hardlim
MSE:
777.9347687904252
RMSE:
27.89
Testing kernel: rbf(0.1)
MSE:
869.9225979470935
RMSE:
29.49

Layer: 19
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
813.4100798541723
RMSE:
28.52
Testing kernel: tribas
MSE:
767.6383294839883
RMSE:
27.71
Testing kernel: hardlim
MSE:
776.5430860398036
RMSE:
27.87
Testing kernel: rbf(0.1)
MSE:
887.8682689933183
RMSE:
29.80

Layer: 20
Testing kernel: default
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tanh
MSE:
815.3424256664553
RMSE:
28.55
Testing kernel: tribas
MSE:
741.3148597563769
RMSE:
27.23
Testing kernel: hardlim
MSE:
791.7435430180153
RMSE:
28.14
Testing kernel: rbf(0.1)
MSE:
897.6456389410098
RMSE:
29.96
Optimal model:
ELMRegressor(hidden_layer=SimpleRandomHiddenLayer(activation_args=None,
                                                  activation_func='tanh',
                                                  n_hidden=1, random_state=0),
             regressor=None)
bestHiddenLayerCount:
1
MSE: 721.58
RMSE: 26.86
"Training LiPo Voltage prediction model"
Training using KDB+ input!
Using PCA!
Number of hidden layers: 1
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 2
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 3
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 4
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 5
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 6
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 7
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 8
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 9
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 10
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 11
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 12
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 13
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 14
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 15
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 16
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 17
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 18
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 19
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)

Number of hidden layers: 20
Training kernel: default
Training kernel: tanh
Training kernel: tribas
Training kernel: hardlim
Training kernel: rbf(0.1)


Testing all kernels...
testX
            0         1         2         3         4         5         6         7         8
0    1.963257  0.046713 -0.077259  0.366586  1.090441  1.193645 -0.936249  0.929530  1.248984
1    1.820846 -0.137200 -0.145260 -0.188178 -0.601863  0.131458 -0.862998  1.010556  1.566087
2    1.759380  0.319093  0.046720 -0.075183 -0.335585  0.171882 -0.891753  0.815953  1.636791
3    1.638533 -0.293103 -0.393625  0.432746  1.240912 -0.233561 -0.468546 -0.183464  1.663764
4   -0.595744 -1.284011 -0.134928 -1.080948  1.041823 -1.398831  0.207357 -1.280912 -0.436476
5   -0.663046 -3.670887  0.028758  1.037113  1.900612  0.660291  2.124652 -1.642483 -1.051388
6   -0.721709 -4.102695  0.417646 -0.058096 -0.060003 -0.131120  3.085435 -1.741736 -0.586513
7   -0.356008 -4.225179  1.319137 -0.545419 -1.107633  0.376932  3.850654 -1.064631 -0.391890
8   -0.652006 -3.887541  1.542945 -0.718951 -0.986917  0.329752  3.130813 -0.596903 -0.544195
9   -0.601155 -3.973821  0.721396 -1.041898 -2.159983 -0.149489  2.493648 -0.319580 -0.489641
10  -0.550648 -2.270812  1.484889  0.676874 -1.181367  2.880539  2.123020  0.035276 -1.121609
11  -0.573705 -2.048343  0.559245 -0.720309 -2.306073  0.629995  1.610163 -0.066240 -0.632503
12  -1.658241  0.477938  0.238141 -0.280779  0.439202 -0.551000  0.612488  0.533854 -1.340843
13  -1.621604  1.070073 -0.148641  1.402786  1.987700  3.643878  0.210725  0.834824 -2.528177
14  -1.764565  0.992391 -0.087975  0.585074  0.535218  0.365566  0.011223  0.412435 -1.689057
15  -1.884408  1.066873 -0.541982 -0.087109 -0.221615 -0.668500 -0.004144  0.083193 -1.330966
16  -1.957340  1.254190 -0.567134  0.256835 -0.179399 -0.380715 -0.281868 -0.105076 -1.424251
17  -2.143967  1.678234 -0.923777  0.419111 -0.649921 -0.449536 -0.524411 -0.192404 -1.465150
18  -2.352407  1.828870 -1.896115  0.285067 -1.124080 -1.070915 -0.939850 -0.276190 -1.464812
19  -2.548299  2.141586 -1.682757  1.714622 -0.999334  0.656354 -0.589983 -0.179306 -1.704567
20  -2.617794  2.117389 -1.918608  0.708982 -1.795187 -0.364791 -0.447221 -0.210529 -1.394819
21  -2.581515  2.009793 -1.994849  0.817437 -1.863172 -0.037223 -0.340307 -0.289137 -1.399095
22  -2.122699  0.890830 -2.704846  0.825136 -2.326010 -0.146304 -0.971654 -0.526649 -1.375923
23  -0.956992 -0.814994 -0.653641 -0.707382 -1.564431  0.130784  1.178103  0.003726 -0.637137
24  -0.649780 -0.984908 -1.310584 -0.221503 -1.502159 -0.075833  0.307064 -0.018481 -0.713542
25  -0.431606 -0.808359 -0.946667  0.377612 -1.486569  0.142043  0.369592  0.079887 -0.387429
26   0.099113 -0.565843 -1.030548 -0.418300 -2.070951 -0.248664  0.383110  0.489317  0.292386
27   0.837830 -0.274944 -0.093781  0.738437 -1.295072  1.258577  0.097887  1.107548  0.480173
28   1.023092 -0.409234 -0.176788  0.190300 -1.665191  0.288117  0.113985  1.144650  0.935367
29   1.567294 -0.446315 -0.188577  0.536049 -0.396612  0.031401  0.052222  0.976038  1.509692
30   1.701565 -0.380201  0.022444  0.425388 -0.658575  0.263513  0.004110  0.811246  1.828693
31   1.887698 -0.712130 -0.501241  0.005786 -0.646554 -0.358989 -0.892638  0.318199  1.878779
32   1.735304 -0.600048  0.208062  0.817628  1.138376  0.730977 -0.254520 -0.295262  1.857272
33   1.797714 -0.556703 -0.356747  0.513486 -0.263336  0.186219 -0.570755  0.391100  1.806561
34   1.643152 -0.249788 -0.517011  0.377220 -0.220484 -0.058206 -0.331902  0.481949  1.885559
35   1.515031 -0.025813 -0.413369  0.131714  0.506177 -0.044654 -0.028879 -0.432175  2.132597
36   1.737377 -0.397222 -0.297646  0.159404 -0.225799  0.400032 -0.638166  0.214422  1.748444
37   1.970618 -0.118560  0.139737  1.191425  0.809509  4.290916 -0.454072  1.027039  0.734966
38   1.739349 -0.150187 -0.064656  0.410753  0.053326  0.927375 -0.196374  0.537010  1.556626
39   0.969700 -0.183221 -0.006793 -1.035956  0.271172  0.019424  0.035123  0.506622  0.677499
40   0.176087 -0.477104  0.225898 -1.547137  1.723391 -0.802870 -0.081015 -0.127773 -0.301735
41  -0.490111 -0.401324  0.198492 -1.716383  1.867329 -0.936952 -0.129674 -0.482730 -0.960790
42   0.506468 -0.115496 -1.956580  0.336211 -1.229481  0.631374 -0.038252 -1.860092  1.381444
43   0.876106 -0.006692 -1.854258 -0.825755 -1.752240  0.062885  0.228647 -1.809908  2.058645
44   1.237466  0.468568 -0.786433  0.761165 -1.707863  1.830679  0.171798 -0.283353  1.840934
45   1.259216  0.361378 -0.640839  0.223868 -1.255909  0.382344  0.225132 -0.190293  2.196488
46   1.620124  0.456627  1.172904 -0.201064 -0.745104  0.555836  0.043739  0.402071  1.982016
47   1.783836  0.377565  0.816333  0.152418 -0.371675  0.305106 -0.548998  1.594919  1.519517
48   1.919608 -0.450489  0.498769  0.579103  0.091301  0.162097 -0.826666  0.906044  1.610622
49   2.027144  0.024849  0.702706  1.847320  3.157067  3.911085 -0.764015  0.848886  0.633529
50   1.620679  0.232969  0.776228  1.121824  0.262478  1.159411 -0.447826  0.192622  1.674248
51   1.763294 -0.093935  0.728352  0.521961  0.318103  0.224147 -0.648235  0.258043  1.790155
52   1.720771  0.251259  0.729237  0.505781  0.039257  0.195194 -0.605591  1.145001  1.571796
53   1.812989 -0.065269 -0.088626  0.840114  0.259794 -0.161686 -1.266493  0.657805  1.639257
54   1.480770 -0.422935 -0.137133 -0.186942  0.443603 -0.782872 -0.603415 -0.247914  1.571073
55  -0.856804 -2.315403  0.708840  0.004073  2.656191  0.107588  0.914497 -2.067283 -0.959439
56  -0.952038 -3.562352 -0.428275 -0.556591  1.822729 -0.965515  1.844270 -1.894482 -0.807194
57  -0.413908 -4.315333 -0.390155 -0.884224 -0.302300 -0.179126  2.962668 -1.679049 -0.567541
58  -2.444197  1.993040 -1.842013  1.414295 -0.956444  1.555623 -0.607401  0.048916 -1.985861
59  -2.582039  1.913341 -2.022446  1.140289 -1.923580 -0.238427 -0.504689 -0.271998 -1.401228
60  -2.104834  0.821854 -2.157193  1.818278 -2.112376 -0.179552 -0.968170 -0.407329 -1.391341
61  -1.525958 -0.462099 -2.520231  2.563269 -2.020214 -0.283274 -1.882391 -0.670629 -1.491238
62  -0.736076  0.579637 -2.463121  1.591436 -2.623801 -0.635933 -1.090818 -0.446784 -0.270300
63   1.428669  1.342621  0.285796  2.318759 -1.492187  2.727332 -0.722273  0.088085  1.208980
64   1.652255  1.054887  0.263157  1.245615 -2.290683  0.958400 -0.736789 -0.028028  1.907721
65   1.956961  1.167764  0.879140  0.839638 -1.905388  0.913967 -0.665255  0.603750  2.099513
66   2.144730  0.255862  0.562448  0.933083 -0.326823  0.157007 -1.417007  0.884292  1.707414
67   1.965039  0.105670  0.416796 -0.209738 -0.831489 -0.355535 -0.561281  0.838645  2.052250
68   2.019911  0.279541  1.205474  1.629886  0.410673  2.013084 -0.327781  2.187617  1.154239
69   2.036518 -0.248568  1.045981  0.555397 -0.213410  0.167027 -0.489980  1.222578  1.749057
70   2.171238 -0.592842  1.418537 -0.043779  0.716731 -0.286700 -0.309712  1.809954  1.667911
71   2.104122 -0.432227  1.061246  0.380754  0.587575 -0.254637 -0.603557  2.307260  1.449849
72   2.199212 -0.846540  0.250446 -0.144523  0.744289 -1.186739 -1.046040  2.036361  1.488268
73   2.139467 -0.885094  0.094016  1.516122  1.828659  1.312854 -1.031349  1.570753  0.997722
74   2.106050 -0.681149  0.252217 -0.672329 -0.512263 -0.009414 -0.786766  1.642594  1.493258
75   1.877088  0.002342 -0.751091 -0.121107 -0.748779  0.144303 -0.948287  1.840685  1.318589
76  -0.310812 -0.021387 -0.890944 -1.134420  0.767206 -0.866441 -0.421247 -0.144239 -0.297966
77  -0.709642 -0.703689 -0.487211 -0.987505  1.921873 -0.771659 -0.216192 -1.207005 -0.926748
78   1.918280 -0.604630 -0.826932  0.443944  0.508067  1.882100 -0.635063  0.354113  1.342324
79   1.850983 -0.369429 -0.416925  0.225992  0.934590  2.262807 -0.316249  0.032001  1.394576
80   1.715179 -0.408014 -0.924090 -0.553301 -0.132274 -0.063300 -0.444527 -0.059014  1.951383
81   1.587420 -0.234916 -0.600078 -0.314300 -0.707614  0.348072 -0.240124 -0.207638  2.007065
82   1.846736 -0.685933 -0.345600 -0.298292  0.391915 -0.035978 -0.694820 -0.413071  1.942818
83   1.778999 -0.579382 -0.990692 -0.863856  0.138267 -0.900556 -0.351885  0.187083  2.032011
84   0.545948 -0.307753  0.864718  0.287351  2.259882  1.108752 -0.142823  0.489476 -0.421101
85  -0.026470 -0.356061  0.720009 -0.711180  1.498493 -0.894480 -0.017895  0.044125 -0.338881
86  -0.742356 -0.406710  0.836262 -0.504813  2.092916 -1.095749  0.017284 -0.388465 -1.071745
87  -0.896989 -0.177221  1.005207 -0.515745  1.606056 -0.973750  0.081295 -0.156201 -1.248112
88  -0.906999 -0.309919  0.180802 -1.443020  0.990316 -1.533631 -0.165065 -0.225015 -1.191383
89  -1.144371  0.485856 -0.017634  0.310260  1.711265  0.899668 -0.613776  0.096185 -2.087501
90  -1.479706  0.887705 -1.059088 -0.995855  0.621513 -1.150207 -0.730101 -0.138119 -1.671037
91  -0.847669 -1.130604 -3.049623 -0.089714 -1.001756 -1.202618 -1.015763 -1.783179 -0.668889
92  -1.032795 -0.685603 -1.687283  1.930830 -0.042809  1.618914 -0.288307 -1.932601 -1.082139
93   2.126778 -0.114514 -1.254058 -1.032661 -0.337372 -0.840885 -1.202508  0.485389  1.943812
94   2.203153  0.062942  0.505964  0.827974  0.835834  1.797269 -0.646533  1.550013  1.198382
95   2.136630 -0.505322  0.873278 -0.106232 -0.001676  0.126070 -0.481559  1.031898  1.780772
96   2.215508 -0.523619  1.251233 -0.286977  0.947470 -0.310440 -0.431437  1.335107  1.727591
97   2.165350 -0.512800  1.470109 -0.077262  0.276995 -0.030175 -0.413891  2.378766  1.474896
98   2.064557 -0.491510  0.546752 -0.655232  1.028746 -1.271451 -0.290378  2.500423  1.609245
99   2.097027 -0.705735  0.684146  1.755345  1.737433  1.437960 -0.745857  1.839402  1.040797
100  2.068484 -0.652713  0.903444  0.492111  0.805267 -0.205382 -0.822396  1.042283  1.621808
101  1.884490 -0.260237  0.777357  0.622989 -0.006398  0.066005 -0.759833  1.272412  1.592377
102 -0.126047 -0.109151  0.472848  0.553973  1.166932 -0.681129 -0.499122  0.129457 -0.142674
103 -0.774792 -0.832463 -0.052678 -0.479046  1.764330 -1.778834 -0.412333 -1.311534 -0.660510
104 -0.761237 -0.614979  0.600043  1.213333  2.210651  1.624336 -0.028366 -0.663377 -1.555733
105 -0.812964 -0.775017 -0.148493 -0.418746  0.978204 -0.751909 -0.142594 -0.950810 -0.972992
106 -0.813342 -0.712129 -0.358390 -0.847054  0.641697 -0.659661 -0.043526 -0.781901 -1.004635
107 -0.809151 -0.765893 -0.648651 -1.238719  0.253988 -0.720289  0.039411 -0.625579 -0.993357
108  1.764848 -0.513602 -0.780651 -0.259198 -0.872026  0.208248 -0.106824  0.824445  1.777080
109  1.716509 -0.110186 -0.008891 -0.052322 -0.586853  0.248171 -0.242004  0.703200  1.802113
110  1.631872 -0.172408 -0.758181 -0.256064 -0.218178 -0.756115 -0.282032  0.334071  2.079773
111  1.746693 -0.378640  0.608924  1.532649  1.144364  1.942024 -0.262791  0.000799  1.533919
112  1.607965 -0.237078 -0.024187  1.090029 -0.093820  0.056551 -0.330594  0.516482  1.876569
113  1.589345 -0.451679  0.008110  1.105394  0.728775 -0.078099 -0.414592 -0.696983  2.133254
114  1.555297 -0.181396  0.469631  0.905595 -0.353243  0.265134 -0.290771  0.459537  1.862567
115  1.708460 -0.453129 -0.026855  0.018020 -0.474594 -0.534435 -0.359500  0.347007  2.071962
116  0.640852 -0.297448  1.343569  1.297537  1.884779  1.329859 -0.108778  0.610094 -0.175031
117  0.080130 -0.463721  1.360598  0.014415  1.896764 -0.833622 -0.081970 -0.071656 -0.239663
118 -0.576396 -0.390055  1.211391 -0.325840  2.017058 -0.976949 -0.130697 -0.433639 -0.901884
119 -0.826644 -0.359815  0.877579 -0.748708  1.679327 -0.971505  0.025278 -0.202801 -1.243280
120 -0.849236 -0.331314 -0.158408 -2.023022  0.915073 -1.653064 -0.175717 -0.248233 -1.176096
121 -0.973139  0.334822 -0.367631 -0.028416  1.639421  0.890066 -0.730386  0.021439 -2.061374
122  1.200469  0.347365 -1.811048 -0.483966 -2.126360 -0.025127  0.196727 -0.545003  2.287643
123  1.540259  0.642649  0.437108  0.803222  0.077006  2.304409  0.125533  0.170219  1.630627
124  1.618000  0.329123  1.026911 -0.155434 -0.786240  0.543000 -0.032392  0.406043  1.960313
125  1.734848  0.508980  0.898280  0.203571 -0.275960  0.116727 -0.416681  1.573206  1.603377
126  1.855775 -0.351238  0.605454  0.714350  0.079095  0.014929 -0.746562  0.906052  1.669599
127  1.873943 -0.212552  0.150471 -0.165806  0.985654 -1.119693 -0.955292  0.416202  1.895089
128  1.784861  0.253777  0.520886  2.112809  1.460959  1.564212 -0.832014  1.045410  1.195103
129  1.838496 -0.193804  0.926837  0.820206 -0.122092  0.116089 -0.972650  0.925070  1.588162
130  1.799224 -0.139405  0.246216  1.286998  0.076062 -0.060656 -1.346749  0.714016  1.616542
131  1.386977 -0.138397  0.483920  1.010254  0.804947 -0.256464 -0.418692  0.003510  1.456193
132 -0.696624 -1.614542  0.421849 -0.381425  1.565947 -1.524704  0.393512 -1.686045 -0.348917
133 -0.625139 -3.873644  0.830980  1.762967  1.662903  1.540432  2.398418 -1.628983 -1.167776
134 -0.602665 -4.186977  0.278418 -0.145305 -0.167835 -0.185870  3.019910 -1.645801 -0.581475
135 -0.632692 -3.895151  1.087906 -1.501451 -1.232723  0.375402  3.808775 -1.061570 -0.560777
136 -0.568299 -3.886361  0.632598 -1.838988 -1.232507  0.293138  3.039570 -0.624453 -0.565738
137  0.842331 -0.337613 -0.588931  0.320765 -1.105943  1.752248  0.233310  1.034770  0.330543
138  0.986502 -0.434893 -0.665118 -0.143048 -1.508030  0.140915  0.022326  0.924149  0.870513
139  1.527212 -0.627302 -0.307557  0.104311 -0.124173 -0.062536 -0.074011  0.318799  1.531870
140  1.759552 -0.530891 -0.326531  0.666926 -0.953434  0.219379 -0.142737  1.315082  1.686316
141  1.649709 -0.273310 -0.120537 -0.066166 -0.433922 -0.345672 -0.191008  0.440952  2.019026
142  1.726018 -0.343699 -0.016265  1.553541  0.237370  0.957303 -0.554650  0.542205  1.600381
143  1.829975 -0.804216  0.308332  0.616434  1.236906 -0.228604 -0.456495 -0.633713  2.084668
144  1.757831 -0.489669  0.013584  1.028259 -0.162945  0.079587 -0.609884  0.351917  1.845403
145  1.536468 -0.208275 -0.155002  1.111419  0.500089 -0.109041 -0.281307  0.029359  1.998000
146  1.430861  0.053769 -0.336799  0.378250 -0.306402 -0.518631  0.094469  0.526905  2.103940
147  1.670336 -0.272436  0.501249  1.668329  1.232329  1.044573 -0.183960  0.531423  1.586107
148  1.081635 -0.147958  0.909740  0.453320  0.696602 -0.208683  0.173745  0.272256  1.173567
149  0.196394 -0.337225  1.188012  0.076233  1.827985 -0.817827 -0.062889  0.092282 -0.087318
150 -0.465511 -0.201779  1.389350 -0.115098  2.150330 -1.017283 -0.174114 -0.218336 -0.843168
151 -0.802230 -0.452553  1.010748 -0.759532  1.768924 -1.540263 -0.011652 -0.287794 -1.065693
152 -0.898974 -0.212852  1.305836  0.627363  1.825810  0.014614 -0.166457  0.042130 -1.595904
153 -0.988366 -0.061534  0.350902  0.088734  1.203648 -0.820100 -0.588900  0.006664 -1.567041
154 -1.243806  0.485672 -0.217812 -0.342434  0.905157 -0.976289 -0.711416 -0.031844 -1.638371
155 -2.224512  2.137761 -1.021113 -0.141881  0.768478 -0.963338 -0.625220  0.108895 -1.930572
156 -2.247329  2.762641 -1.502744 -0.234152 -0.207368 -0.637465 -1.013219 -0.046092 -1.925911
157  0.413804 -0.106922  0.909996 -1.035972 -1.225597  1.181905  0.754530  1.660942 -0.252388
158  0.586059 -0.508180  0.043790 -0.012886 -0.990703  0.003674 -0.313495  1.661970 -0.202599
159  0.237196 -0.586257  0.020864 -0.169342 -1.054051 -0.097671  0.022891  1.251190 -0.274434
160 -0.228396 -0.961471 -0.677072  0.873675 -0.427459 -0.453378 -0.560334  0.269185 -0.685880
161 -0.428614 -1.148871 -1.689286  0.746983 -0.464415 -1.165882 -1.094967 -0.454538 -0.731114
162 -0.832522 -1.082514 -1.506987  2.492470  0.046385  0.496596 -1.085024 -1.443561 -1.135741
163 -0.999261 -1.119918 -1.781195  1.946028 -0.658424 -0.354980 -0.963577 -1.968778 -0.788766
164 -1.921473  0.382352 -0.570843 -1.158859 -0.356309 -0.603409  0.156471 -0.632600 -1.814243
165 -1.907969  0.480144 -0.465870 -0.616504 -0.323300 -0.550219  0.131792 -0.691842 -1.784129
166 -1.724163  0.171906 -0.668585 -1.320439 -0.710455 -1.180282  0.107587 -0.720949 -1.519266
167 -1.508349  0.259757 -0.090593  0.602657  0.145914  0.965979 -0.026619 -0.636245 -1.881532
168 -1.489872  0.253706 -0.189057 -0.237243 -0.594162 -0.435956 -0.131659 -0.665658 -1.520040
169 -1.426447  0.273283 -0.347941 -0.418387 -0.618382 -0.454954 -0.098553 -0.545472 -1.482058
170 -1.469667  0.498381  0.139554 -1.130567 -0.561770 -0.346518  0.576963 -0.476748 -1.329387
171 -1.517595  0.428789 -0.208946 -2.304992 -0.804649 -1.045543  0.712939 -0.368838 -1.236536
172 -1.631245  0.093313 -1.826347 -2.707455 -1.232415 -1.589422  0.134088 -0.886870 -1.270065
173 -1.763627  0.241997 -0.505533 -0.599598 -0.506487 -0.501664  0.156163 -0.823805 -1.595560
174 -1.888211  0.821985 -0.742894 -0.308902 -0.427229 -0.624715  0.000255 -0.703297 -1.660883
175 -2.051489  1.007818 -0.715156 -0.206117 -0.393701 -0.639307  0.001369 -0.716068 -1.692354
176 -2.748594  1.711914 -1.200189 -0.929291 -0.436077 -1.526068  0.252452 -0.357282 -1.807546
177 -3.511952  3.712421 -0.092884  0.479743  0.766066  0.707774  1.221768  0.272038 -2.321454
178 -3.639223  3.747003 -0.557413 -0.171643 -0.054301 -0.839678  1.257520  0.084267 -1.835718
179 -3.998347  4.871151  0.293404 -0.714493 -0.134284 -0.618172  2.391067  0.213643 -1.497557
180 -4.111896  5.346268  0.959474 -1.033642 -0.246164 -0.315931  3.148316  0.429291 -1.277042
181 -4.009082  4.775036  0.349301 -1.982629 -0.606569 -1.062401  3.275347  0.440613 -0.997990
182 -2.968059  3.643196  1.518845 -0.647292  0.031115  1.898192  3.217876  0.505676 -1.158822
183 -2.611009  2.347944  0.803592 -1.458215 -0.984704  0.139852  2.542218 -0.174725 -0.848594
184 -2.567555  1.904609  0.855810 -1.756581 -0.797213  0.091219  2.474726 -0.404845 -1.002505
185 -3.089034  2.682406  1.261112 -2.318636 -0.534846  0.057644  3.265749 -0.187621 -1.018636
186 -0.361475  0.234095 -0.414416 -0.847827  0.154498 -0.341675 -0.469684 -0.732966 -0.709145
187 -0.310691  0.147230 -0.287687 -0.608984  0.042282 -0.472482 -0.489214 -0.475001 -0.605256
188 -0.222564  0.052394 -0.124801 -0.550925 -0.059612 -0.512079 -0.377507 -0.306389 -0.496980
189 -0.092398  0.050881 -0.510470 -1.120169 -0.712870 -0.962442 -0.304446 -0.038688 -0.175551
190  0.106647  0.417331  0.243085  0.425516  0.328599  1.118837 -0.461449  0.157186 -0.571163
191  0.015277  0.424770 -0.323249 -0.129351 -0.659775 -0.119218 -0.242517 -0.500464  0.138765
192  0.265290  0.431768 -0.180454 -0.320601 -0.537891 -0.212272 -0.489185  0.070190  0.077482
193  0.273706  0.432326  0.055710 -0.366552 -0.228338 -0.039695 -0.418123  0.275784 -0.067285
194 -0.254896  0.874405 -0.820742 -1.552337  0.014578 -1.333029 -0.700457 -0.356166 -0.328326
195 -1.048621  2.302209 -0.617188  0.077869  0.919311  0.766621 -1.084729 -0.141866 -1.319020
196  1.663207  1.050601 -0.530162 -0.590688  0.373822 -0.268222 -0.702228  0.211327  1.794852
197  1.160351  0.555235  0.360993 -0.520889 -0.118104 -0.245043 -0.542960  1.006420  0.762583
198 -0.127875  0.513054  0.601419 -3.230804  1.300824 -1.929832  0.586216 -0.598037  0.184409
199 -0.057150  0.316665  0.883079 -0.114290  1.199065 -0.832401 -1.024636  0.474955 -0.911015
200 -0.051655  0.324363  0.919250 -0.155843  1.130697 -0.918166 -1.089752  0.532877 -0.926410
201 -0.197175  0.392781  0.938546  0.041307  0.815147 -0.640216 -0.790924  0.243745 -0.793860
202 -0.153907  0.453664 -0.099270 -2.063850 -0.155083 -1.704679 -0.515490  0.176601 -0.311054
203 -0.065705  0.739478  0.466008  0.346725  0.147050 -0.570484 -0.990115  0.386833 -0.558735
204 -0.312807  1.096322  0.533682  0.116203 -0.296026 -0.237910 -0.337408 -0.339371 -0.197743
205 -0.201370  0.861484  0.600799 -0.612260 -0.035012 -0.413708 -0.499912  0.145991 -0.515909
206 -0.070846  0.022946 -0.983840 -2.698868 -0.618261 -1.589520 -0.642091 -0.105439 -0.335291
207  1.601771  0.688864 -0.405526 -0.479952 -0.225960 -0.023887 -0.879796  0.120118  1.385165
208  1.818762  0.697920 -0.124399 -0.414681  0.152262 -0.144832 -0.791595 -0.005700  1.748328
209  1.773228  0.817324 -0.101626  0.041193 -0.155300  0.078315 -0.895041  0.348817  1.660678
210  1.620834  0.943759 -0.367230 -0.839694 -0.348613 -0.666538 -0.577020 -0.003242  2.008760
211  1.771470  0.840795  0.335074  0.763998  0.164505  1.212251 -0.887794  0.579297  1.351922
212  1.617290  0.997348  0.153385  0.022829 -0.354273  0.064628 -0.607551  0.310719  1.773045
213  1.777800  0.677230 -0.290620  0.233214  0.987674 -0.464898 -1.324738 -0.445627  1.746439
214  1.561605  0.594842 -0.916976  0.995998  0.643321 -0.100590 -1.471720 -0.810482  1.714939
215  1.262670  0.700793 -1.043771  1.087207 -0.108892  0.280067 -1.043436 -1.407032  1.928080
216  1.237040  1.049188 -1.002021  1.498085  0.040042  2.470350 -0.721496 -1.161136  1.499124
217  1.319534  1.019427 -0.194761  0.742254 -0.290899  2.027004 -0.489739 -1.141627  1.691379
218  1.342428  0.823587 -0.935102 -0.495269 -0.813671  0.299947 -0.590616 -1.250289  2.064573
219 -1.653049  0.272788 -1.043531 -1.805812 -0.700836 -1.305283  0.075488 -1.115716 -1.562281
220 -1.565224  0.401234  0.200915  0.464505  0.359345  0.985725 -0.009753 -0.840730 -2.101441
221 -1.653613  0.313863  0.055652  0.105041 -0.236771 -0.623774 -0.114390 -1.053628 -1.687989
222 -1.765281  0.582401  0.832527 -0.180943 -0.096812 -0.636774  0.418020 -0.898332 -1.630900
223 -1.785602  0.667010  1.003171  0.064606  0.016344 -0.561593  0.355649 -0.848146 -1.693783
224 -1.784614  0.645208  0.704654 -0.546696 -0.255723 -1.196484  0.191287 -0.802307 -1.620537
225 -1.023166  0.732870  1.038080  1.336408  0.279219  0.753685 -0.472277 -0.481400 -1.590944
226 -0.523077  0.998689  0.987090  0.557263 -0.579749 -0.305388 -0.570299 -0.110775 -0.975413
227 -0.015198  1.222492  1.020617  0.565101 -0.722339 -0.255327 -0.792264  0.240889 -0.626400
228  0.000232  0.723689  0.694608  0.418635 -0.589049 -0.198529 -1.062902  0.533597 -0.811736
229  0.166012  0.353126  0.126656 -1.250934 -1.091447 -0.907070 -0.905000  0.676441 -0.409644
230  0.607039  0.515724  0.061269 -0.061137 -0.227247  0.916971 -0.821787  0.990833 -0.398848
231  2.184667 -0.376771 -0.151780 -0.859478 -0.208939  0.261547 -0.467918  1.661985  1.566772
232  2.192929 -0.669327  0.354734 -1.286281  0.402535 -0.205896 -0.203192  2.161433  1.523860
233  2.131036 -0.514373  0.281447 -0.906457  1.081175 -0.498148 -0.384155  1.951388  1.507995
234  2.155042 -0.084141  0.889709 -0.102000  0.059392  0.020795 -0.709000  2.521840  1.252878
235  2.256559 -0.637455  1.428552  1.304949  1.853804  2.739895 -0.636135  1.363354  0.927741
236  1.961296 -0.520723  0.503443  1.483978  2.069126  0.950233 -0.398994  0.895286  1.416263
237  1.920197 -0.517008  0.604466  0.896218  0.416711 -0.155911 -0.716301  1.348442  1.591306
238  1.816168 -0.241079  0.174207  1.453798  0.537037 -0.317877 -0.974306  1.210708  1.489151
239 -0.679777 -0.155802  1.069321  0.983894  1.846102 -0.731803 -0.392439 -0.201516 -0.696651
240 -0.861524 -0.824632  0.435028  0.176419  1.566730 -1.590189 -0.260031 -1.427594 -0.604942
241 -0.807511 -0.696161  1.280880  2.243242  2.258242  1.693112 -0.131697 -0.723663 -1.539833
242 -0.897624 -0.703941  1.035278  1.033148  1.090339 -0.747201 -0.173022 -0.787105 -0.940386
243 -0.896741 -0.687594  0.885225  1.059972  0.895203 -0.634676 -0.135159 -0.518172 -1.018776
244 -0.855197 -0.935850  0.571527  0.721437  0.616572 -0.294028  0.089660 -0.381392 -1.069394
245 -0.874962 -0.943902 -0.353812 -1.051903  0.030360 -1.214221  0.167160 -0.522604 -0.870749
246 -0.731897 -1.484643  0.278230 -0.191480  1.050512  1.810713  0.812277 -0.394305 -1.537930
247  1.831875 -0.373789 -1.136052 -0.695999  0.009792 -0.117670 -0.254584  0.295686  1.874107
248  1.763497 -0.217104 -0.953009 -0.297767 -0.602469  0.057157 -0.478987  0.907427  1.713201
249  1.741064 -0.715425 -0.902639 -0.901086  0.556519 -0.853466 -0.300997 -0.554812  2.250212
250  1.895405 -0.648475 -0.216060  1.361634  0.944197  1.807981 -0.775012  0.038493  1.408189
251  1.605928 -0.273031 -0.162464  0.355058 -0.346253  0.174867 -0.386834 -0.412240  2.071304
252  1.663531 -0.409862  0.250069  0.584232  0.329696  0.397766 -0.291423 -0.355656  1.996452
253  1.683903 -0.639655 -0.005596  0.552417  0.724695 -0.086244 -0.252529 -0.408294  2.084497
254  1.587105 -0.015644 -0.333109 -0.191890 -0.420154 -0.604684 -0.063678  0.503886  1.965338
255  0.368659 -0.332467  1.201575  0.704755  2.494112  0.751710 -0.088811  0.227947 -0.383555
256 -0.128864 -0.397317  0.884954 -0.588829  1.764464 -0.923961 -0.052532 -0.137702 -0.449088
257 -0.764197 -0.313836  0.511225 -1.096731  2.070642 -1.133726  0.069764 -0.415784 -1.128944
258 -0.821447 -0.334484  0.283006 -1.434619  1.439381 -0.964322 -0.056099 -0.308878 -1.293497
259  1.555613  0.535437 -0.166020 -0.206774 -0.611374  2.072278  0.212536  0.848271  1.529582
260  1.828616  0.249454 -0.226579 -1.256725 -0.805031  0.208416 -0.230639  1.372480  1.675183
261  1.927745  0.023125 -0.370545 -1.146403 -0.925617  0.194189 -0.986497  1.408905  1.458237
262  1.931417 -0.152767 -0.728711 -1.050563  0.384024 -0.239365 -0.670414  0.474674  1.733219
263  1.690450  0.166190 -0.564314 -0.973357 -0.314602  0.271403 -0.459926  0.050355  1.850086
264 -0.351728 -4.359139 -0.201165 -1.157946 -0.573451  0.117099  3.356078 -1.293843 -0.603139
265 -0.921225 -2.870430  0.625988  0.394442 -0.211577  1.409099  1.672795 -0.751463 -1.409871
266 -0.936133 -2.863006  0.582215 -0.207536 -1.182170  0.101786  1.821083 -0.605002 -0.949834
267 -0.882416 -2.627865  0.754587 -0.015631 -1.397480  0.199673  1.732867 -0.669054 -0.851824
268 -0.788254 -2.232785  1.199773 -0.024884 -1.338128  0.389265  1.590756 -0.649700 -0.861698
269 -0.838032 -2.061774  0.436334 -0.772245 -1.654086 -0.464824  1.512297 -0.703591 -0.602399
270 -0.686717 -1.363297  1.286054  1.250034 -0.299833  2.658837  1.133388 -0.328556 -1.409396
271 -0.731417 -1.159555  0.502603  0.790502 -0.912597  0.187316  0.532446 -0.251554 -1.038687
272 -0.702108 -0.583793 -0.098177  0.978150 -1.080515 -0.157153  0.268800 -0.198160 -0.846617
273 -0.579846 -0.719091 -1.868122 -1.670421 -1.879516 -1.552204 -0.044594 -0.264869 -0.568395
274 -0.822293  0.298661  1.469160  0.283711 -0.173114  1.459518  1.414598  0.642140 -1.001351
275 -0.851076  0.168958  0.514388 -1.181398 -0.996521 -0.022111  1.276051  0.372522 -0.656253
276 -1.477533  0.072661 -0.662683 -0.699339 -0.329698 -0.536015 -0.093946 -0.558289 -1.493179
277 -1.723286 -0.025282 -1.198763 -0.738694 -0.384678 -1.411720 -0.433614 -0.676025 -1.633762
278 -2.084863  0.244662 -0.519747  1.393177  0.850646  0.691004 -0.456023 -0.671180 -2.564322
279 -1.981738  0.476351 -1.061952  0.990063  0.238362 -1.010768 -0.580335 -0.939251 -2.277664
280 -1.796894  0.142384 -0.792701  0.999988 -0.056755 -0.841864 -0.632661 -1.134933 -2.334956
281 -1.764569  0.489101 -0.002608  0.572657 -0.061664 -0.613924 -0.006587 -1.296460 -1.956848
282 -1.774233  0.328051 -0.434450 -0.213288 -0.476417 -1.276967 -0.188342 -1.266728 -1.847368
283 -1.660363  1.266206  1.103690  0.851705  0.560543  1.154757  0.549020 -0.688090 -2.145187
284 -1.481830  0.684480  0.940310 -0.055617 -0.271085 -0.313368  0.442493 -0.901849 -1.688705
285 -1.426582  0.091916  0.820083 -0.003150 -0.253470 -0.403964 -0.024933 -0.850257 -1.880100
286 -1.568324 -0.333632 -0.995727 -2.646360 -0.963242 -1.762892 -0.013689 -1.070848 -1.580164
287 -1.766830 -0.312232  0.113704  0.485197  0.460225  0.866479 -0.020462 -0.861646 -2.358158
288 -1.780770 -0.324927 -0.529572 -0.688070 -0.103606 -0.383093 -0.038299 -0.947122 -2.050300
289 -4.256423  4.339812 -0.050584 -1.343525 -0.008389 -0.707147  2.768297  0.384233 -1.663427
290 -3.717248  4.966842  2.156727 -2.454106 -0.609599  0.021360  4.740023  0.710900 -0.314448
291 -4.344890  4.940831  2.076931 -1.764375 -0.002595 -0.068273  4.525565  0.952553 -1.108831
292 -4.697081  5.309009  1.911997 -2.851777 -0.153778 -0.971425  5.005420  0.610047 -0.890170
293 -1.371218 -0.467085 -0.411416  1.618959 -0.558153  1.272310 -0.225339 -1.113450 -1.664101
294 -1.657949  0.133003 -0.018615  0.401105 -1.045919 -0.136487  0.411489 -1.213842 -1.222039
295 -1.576536  0.194200  0.001433  0.446438 -0.834261 -0.193406  0.394482 -1.234954 -1.151355
296 -1.367168  0.398311 -0.141587  0.554278 -0.970616 -0.198606  0.206325 -0.785502 -1.000158
297 -1.150519  0.165318 -0.333874 -0.357662 -1.279370 -0.835597  0.087344 -0.710602 -0.802391
298 -0.912993 -0.157806 -0.676646  2.103732 -0.390466  1.238907 -0.939577 -0.620741 -1.512786
299 -0.889815 -0.170684 -1.043521  1.273812 -1.049334 -0.353163 -1.006075 -0.760665 -1.033083
300 -0.694232  0.474548  0.330174  0.485878 -1.150645  0.241620  0.263132 -0.539860 -0.377473
301 -0.050401  0.924673  0.613468 -0.456332 -1.611355  0.351337  0.490548  0.247654  0.217576
302  0.606256  0.786061 -0.562384 -1.798174 -2.062810 -0.491617 -0.052085  0.880643  0.621100
303 -0.545853  0.344518 -0.174747 -0.604373 -0.846472 -0.181560 -0.097579  0.364382 -0.813005
304 -0.092099  0.349749 -0.708284 -2.064983 -0.970983 -1.353355 -0.139484  0.505571 -0.142115
305 -1.536137  0.598287  0.400196  1.201145  1.142277  2.226932  0.160261 -0.033243 -2.282461
306 -1.641311  0.458886  0.432572  0.771373  0.365329  0.316520  0.039215 -0.277056 -1.861055
307 -1.673005  0.293645  0.276688  0.272429 -0.035216 -0.592063 -0.071781 -0.555712 -1.702376
308 -1.736968  0.367011  0.218260  0.332244 -0.061389 -0.613903 -0.226923 -0.680493 -1.784866
309 -1.905678  0.248061 -1.102444 -1.872880 -0.544067 -1.719932  0.049097 -0.874519 -1.705980
['default', 'tanh', 'tribas', 'hardlim', 'rbf(0.1)']

Layer: 1
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.0531911793727031
RMSE:
0.23
Testing kernel: tribas
MSE:
0.04867370249295531
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.05528314402755089
RMSE:
0.24
Testing kernel: rbf(0.1)
MSE:
0.047558881115834
RMSE:
0.22

Layer: 2
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.06180504961788171
RMSE:
0.25
Testing kernel: tribas
MSE:
0.04376278220254543
RMSE:
0.21
Testing kernel: hardlim
MSE:
0.060305708745666875
RMSE:
0.25
Testing kernel: rbf(0.1)
MSE:
0.04762608055243752
RMSE:
0.22

Layer: 3
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.05550666526932603
RMSE:
0.24
Testing kernel: tribas
MSE:
0.04240951787731777
RMSE:
0.21
Testing kernel: hardlim
MSE:
0.059628128112283844
RMSE:
0.24
Testing kernel: rbf(0.1)
MSE:
0.04520833713245799
RMSE:
0.21

Layer: 4
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.053717654280931684
RMSE:
0.23
Testing kernel: tribas
MSE:
0.05185057221546438
RMSE:
0.23
Testing kernel: hardlim
MSE:
0.051614632272706386
RMSE:
0.23
Testing kernel: rbf(0.1)
MSE:
0.047284094047568524
RMSE:
0.22

Layer: 5
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.04767115941003752
RMSE:
0.22
Testing kernel: tribas
MSE:
0.04822193405768281
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.049416268379377216
RMSE:
0.22
Testing kernel: rbf(0.1)
MSE:
0.05111172333336255
RMSE:
0.23

Layer: 6
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.05418901053012992
RMSE:
0.23
Testing kernel: tribas
MSE:
0.049328803573151586
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.05157192648903867
RMSE:
0.23
Testing kernel: rbf(0.1)
MSE:
0.051131326900834606
RMSE:
0.23

Layer: 7
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.04766318860723878
RMSE:
0.22
Testing kernel: tribas
MSE:
0.05296609561417232
RMSE:
0.23
Testing kernel: hardlim
MSE:
0.05160187580714132
RMSE:
0.23
Testing kernel: rbf(0.1)
MSE:
0.05083382151060045
RMSE:
0.23

Layer: 8
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.05489505867660359
RMSE:
0.23
Testing kernel: tribas
MSE:
0.04402087941126908
RMSE:
0.21
Testing kernel: hardlim
MSE:
0.048648052605010955
RMSE:
0.22
Testing kernel: rbf(0.1)
MSE:
0.051285290934412246
RMSE:
0.23

Layer: 9
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.06024256654667528
RMSE:
0.25
Testing kernel: tribas
MSE:
0.04410516532967746
RMSE:
0.21
Testing kernel: hardlim
MSE:
0.05502202261298331
RMSE:
0.23
Testing kernel: rbf(0.1)
MSE:
0.05199573005713248
RMSE:
0.23

Layer: 10
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.0579854663729528
RMSE:
0.24
Testing kernel: tribas
MSE:
0.047574774032138466
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.059025028376314716
RMSE:
0.24
Testing kernel: rbf(0.1)
MSE:
0.050908419659565794
RMSE:
0.23

Layer: 11
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.05943573379322196
RMSE:
0.24
Testing kernel: tribas
MSE:
0.04425141809751545
RMSE:
0.21
Testing kernel: hardlim
MSE:
0.05739353974780175
RMSE:
0.24
Testing kernel: rbf(0.1)
MSE:
0.0493607519108048
RMSE:
0.22

Layer: 12
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.052222380631261035
RMSE:
0.23
Testing kernel: tribas
MSE:
0.04411055358314971
RMSE:
0.21
Testing kernel: hardlim
MSE:
0.05059313102235692
RMSE:
0.22
Testing kernel: rbf(0.1)
MSE:
0.049301150306142856
RMSE:
0.22

Layer: 13
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.04585482081042189
RMSE:
0.21
Testing kernel: tribas
MSE:
0.0480430689098109
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.0497891837483715
RMSE:
0.22
Testing kernel: rbf(0.1)
MSE:
0.049087840571946505
RMSE:
0.22

Layer: 14
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.04716258377715646
RMSE:
0.22
Testing kernel: tribas
MSE:
0.04934984459285225
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.04733294174527175
RMSE:
0.22
Testing kernel: rbf(0.1)
MSE:
0.048943937110518726
RMSE:
0.22

Layer: 15
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.04599683166947566
RMSE:
0.21
Testing kernel: tribas
MSE:
0.04641468871580436
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.049891481266056234
RMSE:
0.22
Testing kernel: rbf(0.1)
MSE:
0.04957828530257716
RMSE:
0.22

Layer: 16
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.05236904170205447
RMSE:
0.23
Testing kernel: tribas
MSE:
0.050080536508731
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.051281029095468215
RMSE:
0.23
Testing kernel: rbf(0.1)
MSE:
0.05103970755876654
RMSE:
0.23

Layer: 17
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.05300968181537849
RMSE:
0.23
Testing kernel: tribas
MSE:
0.052047242401810793
RMSE:
0.23
Testing kernel: hardlim
MSE:
0.05283222244421887
RMSE:
0.23
Testing kernel: rbf(0.1)
MSE:
0.05100161708985213
RMSE:
0.23

Layer: 18
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.05367237879977976
RMSE:
0.23
Testing kernel: tribas
MSE:
0.04584726776598905
RMSE:
0.21
Testing kernel: hardlim
MSE:
0.05605203987994634
RMSE:
0.24
Testing kernel: rbf(0.1)
MSE:
0.050923618940033076
RMSE:
0.23

Layer: 19
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.058916812084983165
RMSE:
0.24
Testing kernel: tribas
MSE:
0.058641856579516864
RMSE:
0.24
Testing kernel: hardlim
MSE:
0.05436636666386796
RMSE:
0.23
Testing kernel: rbf(0.1)
MSE:
0.05017956979784317
RMSE:
0.22

Layer: 20
Testing kernel: default
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tanh
MSE:
0.04761926259497266
RMSE:
0.22
Testing kernel: tribas
MSE:
0.04974055488964555
RMSE:
0.22
Testing kernel: hardlim
MSE:
0.05119100386618484
RMSE:
0.23
Testing kernel: rbf(0.1)
MSE:
0.05083378812128043
RMSE:
0.23
Optimal model:
ELMRegressor(hidden_layer=SimpleRandomHiddenLayer(activation_args=None,
                                                  activation_func='tribas',
                                                  n_hidden=2, random_state=0),
             regressor=None)
bestHiddenLayerCount:
2
MSE: 0.04
RMSE: 0.21
"Deploying GPS speed prediction model"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
13800 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
"Deploying LiPo Voltage prediction model"
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
11095 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
"Parameters for data synthesis:"
"lowThrottle"
1000
"highThrottle"
2000
"throttleSteps"
10
"Synthesizing GPS speeds and LiPo voltage for different throttle values and t..
`LiPoPredictionTable
`gpsSpeedPredictionTable
"Generating timestep sample 2"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
74091 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
120358 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11352                           11352                      
1      22704                           22704                      
"Generating timestep sample 3"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
116246 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
181016 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11352                           11352                      
1      22704                           22704                      
2      34056                           34056                      
"Generating timestep sample 4"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
185657 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
247427 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11352                           11352                      
1      22704                           22704                      
2      34056                           34056                      
3      45408                           45408                      
"Generating timestep sample 5"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
244968 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
311689 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11352                           11352                      
1      22704                           22704                      
2      34056                           34056                      
3      45408                           45408                      
4      56760                           56760                      
"Generating timestep sample 6"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
309316 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
375595 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11352                           11352                      
1      22704                           22704                      
2      34056                           34056                      
3      45408                           45408                      
4      56760                           56760                      
5      68112                           68112                      
"Generating timestep sample 7"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2010, 1909, 1808, 1707, 1606, 1505, 1404, 1303, 1202, 1101, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
371622 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2010, 1909, 1808, 1707, 1606, 1505, 1404, 1303, 1202, 1101, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
447953 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11352                           11352                      
1      22704                           22704                      
2      34056                           34056                      
3      45408                           45408                      
4      56760                           56760                      
5      68112                           68112                      
6      79464                           79464                      
"Generating timestep sample 8"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2010, 1909, 1808, 1707, 1606, 1505, 1404, 1303, 1202, 1101, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
441979 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2010, 1909, 1808, 1707, 1606, 1505, 1404, 1303, 1202, 1101, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
512858 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11352                           11352                      
1      22704                           22704                      
2      34056                           34056                      
3      45408                           45408                      
4      56760                           56760                      
5      68112                           68112                      
6      79464                           79464                      
7      90816                           90816                      
"Generating timestep sample 9"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2010, 1909, 1808, 1707, 1606, 1505, 1404, 1303, 1202, 1101, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
509648 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2010, 1909, 1808, 1707, 1606, 1505, 1404, 1303, 1202, 1101, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
571414 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11352                           11352                      
1      22704                           22704                      
2      34056                           34056                      
3      45408                           45408                      
4      56760                           56760                      
5      68112                           68112                      
6      79464                           79464                      
7      90816                           90816                      
8      102168                          102168                     
"Generating timestep sample 10"
Using ELM GPS Model
Predicting GPS Speed using KDB+ input!
throttleInputRange
[2010, 1909, 1808, 1707, 1606, 1505, 1404, 1303, 1202, 1101, 1000]
Using PCA!
gpsPredictionPDF set
prediction complete!
579142 4241648
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Using ELM LiPo Model
Predicting LiPo Voltage using KDB+ input!
throttleInputRange
[2010, 1909, 1808, 1707, 1606, 1505, 1404, 1303, 1202, 1101, 1000]
Using PCA!
LiPoPredictionPDF set
prediction complete!
649340 4241216
sys:1: FutureWarning: The 'get_values' method is deprecated and will be removed in a future version. Use '.to_numpy()' or '.array' instead.
Sample gpsSpeedPredictionTableRowCount LiPoPredictionTableRowCount
------------------------------------------------------------------
0      11352                           11352                      
1      22704                           22704                      
2      34056                           34056                      
3      45408                           45408                      
4      56760                           56760                      
5      68112                           68112                      
6      79464                           79464                      
7      90816                           90816                      
8      102168                          102168                     
9      113520                          113520                     
`:lookbackSteps.dat
"Saving synthesizedThrottleLSTMTrainingDataMatrix to disk"
"Saving realThrottleLSTMTrainingDataMatrix to disk"
"Training LSTM (Regression Window) using synthesized data!"
Training using KDB+ input!
Look back steps detected: 10
WARNING:tensorflow:From /Users/foorx/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Epoch 1/100
 - 30s - loss: 0.0070
Epoch 2/100
 - 30s - loss: 0.0036
Epoch 3/100
 - 30s - loss: 0.0034
Epoch 4/100
 - 30s - loss: 0.0033
Epoch 5/100
 - 30s - loss: 0.0033
Epoch 6/100
 - 30s - loss: 0.0033
Epoch 7/100
 - 30s - loss: 0.0033
Epoch 8/100
 - 32s - loss: 0.0033
Epoch 9/100
 - 33s - loss: 0.0033
Epoch 10/100
 - 34s - loss: 0.0033
Epoch 11/100
 - 33s - loss: 0.0033
Epoch 12/100
 - 34s - loss: 0.0033
Epoch 13/100
 - 33s - loss: 0.0033
Epoch 14/100
 - 33s - loss: 0.0033
Epoch 15/100
 - 33s - loss: 0.0033
Epoch 16/100
 - 33s - loss: 0.0033
Epoch 17/100
 - 33s - loss: 0.0033
Epoch 18/100
 - 33s - loss: 0.0033
Epoch 19/100
 - 34s - loss: 0.0033
Epoch 20/100
 - 33s - loss: 0.0033
Epoch 21/100
 - 33s - loss: 0.0033
Epoch 22/100
 - 33s - loss: 0.0033
Epoch 23/100
 - 33s - loss: 0.0033
Epoch 24/100
 - 33s - loss: 0.0033
Epoch 25/100
 - 33s - loss: 0.0033
Epoch 26/100
 - 36s - loss: 0.0033
Epoch 27/100
 - 36s - loss: 0.0033
Epoch 28/100
 - 40s - loss: 0.0033
Epoch 29/100
 - 35s - loss: 0.0033
Epoch 30/100
 - 36s - loss: 0.0033
Epoch 31/100
 - 35s - loss: 0.0033
Epoch 32/100
 - 35s - loss: 0.0033
Epoch 33/100
 - 35s - loss: 0.0033
Epoch 34/100
 - 35s - loss: 0.0033
Epoch 35/100
 - 35s - loss: 0.0033
Epoch 36/100
 - 35s - loss: 0.0033
Epoch 37/100
 - 39s - loss: 0.0033
Epoch 38/100
 - 37s - loss: 0.0033
Epoch 39/100
 - 35s - loss: 0.0033
Epoch 40/100
 - 35s - loss: 0.0033
Epoch 41/100
 - 35s - loss: 0.0033
Epoch 42/100
 - 35s - loss: 0.0033
Epoch 43/100
 - 35s - loss: 0.0033
Epoch 44/100
 - 35s - loss: 0.0033
Epoch 45/100
 - 36s - loss: 0.0033
Epoch 46/100
 - 36s - loss: 0.0033
Epoch 47/100
 - 36s - loss: 0.0033
Epoch 48/100
 - 36s - loss: 0.0033
Epoch 49/100
 - 36s - loss: 0.0033
Epoch 50/100
 - 35s - loss: 0.0033
Epoch 51/100
 - 35s - loss: 0.0033
Epoch 52/100
 - 35s - loss: 0.0033
Epoch 53/100
 - 35s - loss: 0.0033
Epoch 54/100
 - 39s - loss: 0.0033
Epoch 55/100
 - 40s - loss: 0.0033
Epoch 56/100
 - 37s - loss: 0.0033
Epoch 57/100
 - 37s - loss: 0.0033
Epoch 58/100
 - 35s - loss: 0.0033
Epoch 59/100
 - 35s - loss: 0.0033
Epoch 60/100
 - 36s - loss: 0.0033
Epoch 61/100
 - 35s - loss: 0.0033
Epoch 62/100
 - 35s - loss: 0.0033
Epoch 63/100
 - 35s - loss: 0.0033
Epoch 64/100
 - 35s - loss: 0.0033
Epoch 65/100
 - 35s - loss: 0.0033
Epoch 66/100
 - 36s - loss: 0.0033
Epoch 67/100
 - 40s - loss: 0.0033
Epoch 68/100
 - 39s - loss: 0.0033
Epoch 69/100
 - 36s - loss: 0.0033
Epoch 70/100
 - 35s - loss: 0.0033
Epoch 71/100
 - 47s - loss: 0.0033
Epoch 72/100
 - 35s - loss: 0.0033
Epoch 73/100
 - 35s - loss: 0.0033
Epoch 74/100
 - 35s - loss: 0.0033
Epoch 75/100
 - 35s - loss: 0.0033
Epoch 76/100
 - 35s - loss: 0.0033
Epoch 77/100
 - 35s - loss: 0.0033
Epoch 78/100
 - 35s - loss: 0.0033
Epoch 79/100
 - 35s - loss: 0.0033
Epoch 80/100
 - 35s - loss: 0.0033
Epoch 81/100
 - 35s - loss: 0.0033
Epoch 82/100
 - 35s - loss: 0.0033
Epoch 83/100
 - 35s - loss: 0.0033
Epoch 84/100
 - 35s - loss: 0.0033
Epoch 85/100
 - 35s - loss: 0.0033
Epoch 86/100
 - 35s - loss: 0.0033
Epoch 87/100
 - 35s - loss: 0.0033
Epoch 88/100
 - 35s - loss: 0.0033
Epoch 89/100
 - 35s - loss: 0.0033
Epoch 90/100
 - 35s - loss: 0.0033
Epoch 91/100
 - 35s - loss: 0.0033
Epoch 92/100
 - 35s - loss: 0.0033
Epoch 93/100
 - 35s - loss: 0.0033
Epoch 94/100
 - 35s - loss: 0.0033
Epoch 95/100
 - 35s - loss: 0.0033
Epoch 96/100
 - 35s - loss: 0.0033
Epoch 97/100
 - 35s - loss: 0.0033
Epoch 98/100
 - 35s - loss: 0.0033
Epoch 99/100
 - 35s - loss: 0.0033
Epoch 100/100
 - 35s - loss: 0.0033
Finished training Regression (Normal) LSTM!
Using Regression (Normal) LSTM
Generating Rolling Launch Control Throttle sequence using KDB+ input!
Look back steps detected: 10
Rolling Launch Control Throttle generated!
"Transferring newly trained LSTM model to cloud!"
"Completed Updating Models"
9901287 13519230048
q)
